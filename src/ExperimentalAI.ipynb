{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".container {\n",
    "  width: 100%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from AIBaseClass import ChessAI\n",
    "from Exercise03AI import Exercise03AI\n",
    "from Exercise04AI import Exercise04AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 09: Experiment\n",
    "\n",
    "tbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "from typing import Any, Callable\n",
    "\n",
    "\n",
    "class ExperimentalAI(Exercise03AI):\n",
    "    \"\"\"Chooses middle game moves using minimax algorithm, alpha-beta-pruning,\n",
    "    memoization, progressive deepening and the singular value extension\"\"\"\n",
    "\n",
    "    def __init__(self, max_depth: int = 8, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.cache_new: dict[tuple, Any] = {}\n",
    "        self.cache_old: dict[tuple, Any] = {}\n",
    "        self.MAX_SVE_DEPTH: int = max(max_depth, kwargs[\"search_depth\"])\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resets all internal variables\"\"\"\n",
    "        super().reset()\n",
    "        self.cache_new.clear()\n",
    "        self.cache_old.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `debug_minimax` ist eine Debugging-Hilfsfunktion um die Berechnungen der `evaluate_minimax`-Funktion nachvollziehen zu können. Die Funktion wird bei Bedarf als Dekorator auf die Funktion `evaluate_minimax` angewendet, ist aber standardmäßig nicht aktiviert. Die Parameter sind dieselben wie bei der Funktion `evaluate_minimax` und werden im dortigen Abschnitt beschrieben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_minimax(evaluate_minimax: Callable):\n",
    "    \"\"\"Prints the Minimax Game Tree.\"\"\"\n",
    "\n",
    "    def minimax_debug(\n",
    "        self,\n",
    "        minimax: Callable,\n",
    "        board: chess.Board,\n",
    "        current_evaluation: int,\n",
    "        limit: int,\n",
    "        level: int = 0,\n",
    "        alpha: int = -ExperimentalAI.LIMIT,\n",
    "        beta: int = ExperimentalAI.LIMIT,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        color = \"white\" if board.turn else \"black\"\n",
    "        ws = (level + 1) * \"    \"\n",
    "        key = self.get_key(board)\n",
    "        if key in self.cache_new or key in self.cache_old:\n",
    "            print(\n",
    "                f\"level: {level} {ws} mm> {color}; α: {alpha}; β: {beta}; limit: {limit}\"\n",
    "            )  # memoization\n",
    "        else:\n",
    "            print(\n",
    "                f\"level: {level} {ws} --> {color}; α: {alpha}; β: {beta}; limit: {limit}\"\n",
    "            )\n",
    "        evaluation, best_move = evaluate_minimax(\n",
    "            self, minimax, board, current_evaluation, limit, level, alpha, beta\n",
    "        )\n",
    "        print(\n",
    "            f\"level: {level} {ws} <-- {color}; ev: {evaluation}; best_move: {best_move}\"\n",
    "        )\n",
    "        return evaluation, best_move\n",
    "\n",
    "    return minimax_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `is_quiet_move_and_push` ist ein Indikator dafür, ob die Suchtiefe (`limit`) erhöht werden muss. Dabei prüft die Funktion ob der übergebene Zug ein Schlagzug, eine Umwandlung oder ein Schachzug ist und gibt entsprechend `True` bzw. `False` zurück. Der übergebene Zug wird im Laufe der Prüfung auf das Board angewendet.\n",
    "\n",
    "Hinweis: Die `chess`-Bibliothek bietet auch die Funktion `gives_check` an, welche überprüft ob ein übergebener Zug den Gegner in Schach setzt. Da diese Funktion aber intern nur den Zug anwendet, `is_check` aufruft und den Zug anschließend wieder entfernt wurde aus Effizienzgründen direkt letztere Funktion verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentalAI(ExperimentalAI):  # type: ignore\n",
    "    def is_quiet_move_and_push(self, board: chess.Board, move: chess.Move) -> bool:\n",
    "        \"\"\"Checks if the next move was an promotion, capture or check move. Pushes the given move on the board.\"\"\"\n",
    "        if move.promotion or board.piece_type_at(move.to_square):\n",
    "            board.push(move)\n",
    "            return False\n",
    "        board.push(move)\n",
    "        if board.is_check():\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Singular Value Extension erfordert es, dass das Limit für die Suche nun erstmals laufend angepasst werden muss. Um negative Zahlen zu vermeiden, eine statistische Erfassung zu ermöglichen und aus Gründen der Verständlichkeit wird daher der Parameter `depth`, welcher bisher zum Zielwert `0` dekrementiert wurde, nun durch die Parameter `level` und `limit` ersetzt. Der Parameter `limit` gibt dabei die gewünschte Suchtiefe an und kann im Laufe der Berechnung durch die SVE erhöht werden. Das Argument `level` ist nun die aktuelle Suchtiefe, welche bei `0` startet und mit jedem Aufruf von `minValue` oder `maxValue` inkrementiert wird.\n",
    "\n",
    "Für die SVE ist eine neue Abbruchbedingung bei Erreichen der maximalen Tiefe notwendig. Diese wird hier in der Funktion `minimax_early_abort` hinzugefügt. Parallel dazu ermöglicht die Veränderung der Parameter (vorheriger Absatz) einige Vereinfachungen, welche in dieser überschriebenen Variante implementiert sind.\n",
    "\n",
    "Die folgenden Änderungen wurden vorgenommen:\n",
    "- Die Abbruchbedingung wurde auf die neuen Parameter angepasst und lautet nun `level == limit` statt wie bisher `depth == 0`.\n",
    "- Es gibt eine zusätzliche Abbruchbedingung sobald die maximale Tiefe für die Singular-Value-Extension erreicht ist. In diesem Fall wird die Evaluierung des aktuellen Pfades beendet, auch wenn dieser sich noch in einer Kette von nicht-ruhigen Zügen befindet. Da der Pfad nicht vollständig evaluiert wurde und somit ungewiss ist, wird er mit dem jeweils schlechtesten Wert versehen. Somit werden unvollständig untersuchte Pfade immer vermieden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentalAI(ExperimentalAI):  # type: ignore\n",
    "    def minimax_early_abort(\n",
    "        self, board: chess.Board, level: int, depth: int, current_evaluation: int\n",
    "    ) -> int | None:\n",
    "        \"\"\"Returns an evaluation iff the minimax has an early exit condition. Returns None otherwise.\"\"\"\n",
    "        is_checkmate = board.is_checkmate()\n",
    "        if is_checkmate and not board.turn:\n",
    "            # White has won the game\n",
    "            evaluation = self.LIMIT - level\n",
    "            return evaluation\n",
    "\n",
    "        if is_checkmate and board.turn:\n",
    "            # Black has won the game\n",
    "            evaluation = -self.LIMIT + level\n",
    "            return evaluation\n",
    "\n",
    "        if (\n",
    "            board.is_insufficient_material()\n",
    "            or not board.legal_moves\n",
    "            or board.is_fifty_moves()\n",
    "        ):\n",
    "            # Game is a draw\n",
    "            return 0\n",
    "\n",
    "        # Recursion abort case\n",
    "        if depth == 0:\n",
    "            return current_evaluation\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Lesbarkeit der Minimax Funktion zu wahren, wurde diese in die Funktionen `maxValue`, `minValue` und `evaluate_minimax` aufgeteilt, wobei `maxValue` und `minValue` die beiden Zweige des Minimax Algorithmus darstellen.\n",
    "Die `evaluate_minimax` übernimmt nun die Memoisierung, wodurch kein Dekorator mehr benötigt wird.\n",
    "\n",
    "Die Funtion `maxValue` berechnet wie bisher die maximale Evaluierung aller möglichen Züge. Neu hinzugekommen ist die laufende Erhöhung des Limits, welche direkt nach dem Entfernen eines Zuges von der Prioritätswarteschlange `moves` durchgeführt wird. Dort werden nun drei Fälle unterschieden:\n",
    "- Wenn die Ungleichung `level + 1 < limit` erfüllt ist, wird das Limit grundsätzlich nicht erhöht. Dies hat zur Folge, dass die SVE erst zum Einsatz kommt, wenn die Zielsuchtiefe im nächsten Schritt erreicht ist und somit die Suche sonst zu Ende wäre. Diese Bedingung wurde aus Performance-Gründen mit aufgenommen, da die Berechnungen ansonsten durch die teilweise sehr hohe Tiefe wesentlich länger dauern. Des weiteren vereinfacht es den Zugriff auf den Cache im Progressive Deepening (die Berechnung der Tiefe des letzten Durchlaufes).\n",
    "- Wenn der Zug \"ruhig\" ist, d.h. es sich nicht um einen Schlagzug, eine Umwandlung oder ein Schachzug handelt, wird das Limit nicht erhöht.\n",
    "- Falls die ersten beiden Bedingungen nicht zutreffen, wird die SVE durchgeführt und das Limit um `1` erhöht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "\n",
    "class ExperimentalAI(ExperimentalAI):  # type: ignore\n",
    "    def maxValue(\n",
    "        self,\n",
    "        board: chess.Board,\n",
    "        current_evaluation: int,\n",
    "        depth: int,\n",
    "        level: int,\n",
    "        alpha: int,\n",
    "        beta: int,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        \"\"\"Searches the best value with given depth using minimax algorithm\"\"\"\n",
    "        self.stats[-1][\"nodes\"] += 1\n",
    "        early_abort_evaluation = self.minimax_early_abort(\n",
    "            board, level, depth, current_evaluation\n",
    "        )\n",
    "        if early_abort_evaluation is not None:\n",
    "            self.stats[-1][\"leaf_ctr\"] += 1\n",
    "            self.stats[-1][\"depth_sum\"] += level  # for calculating the average depth\n",
    "            return early_abort_evaluation, None\n",
    "\n",
    "        best_move = None\n",
    "        moves: list[tuple[int, int, chess.Move]] = []\n",
    "\n",
    "        # White to play (positive numbers are good)\n",
    "        if level < self.DEPTH - 2:\n",
    "            for i, move in enumerate(board.legal_moves):\n",
    "                board.push(move)\n",
    "                key = self.get_key(board)\n",
    "                board.pop()\n",
    "                old_eval = self.cache_old.get(key, (None, i, None))[1]\n",
    "                heapq.heappush(moves, (-old_eval, i, move))\n",
    "        else:\n",
    "            moves = list(enumerate(board.legal_moves))\n",
    "        maxEvaluation = alpha\n",
    "        while moves:\n",
    "            move = heapq.heappop(moves)[-1]\n",
    "            new_evaluation = self.incremental_evaluate(board, current_evaluation, move)\n",
    "            if depth > 1 or level + 1 == self.MAX_SVE_DEPTH:\n",
    "                new_depth = depth\n",
    "                board.push(move)\n",
    "            elif self.is_quiet_move_and_push(board, move):\n",
    "                new_depth = depth\n",
    "            else:\n",
    "                new_depth = depth + 1\n",
    "            evaluation, _, = self.minimax(\n",
    "                self.minValue,\n",
    "                board,\n",
    "                new_evaluation,\n",
    "                new_depth - 1,\n",
    "                level + 1,\n",
    "                maxEvaluation,\n",
    "                beta,\n",
    "            )\n",
    "            board.pop()\n",
    "            if evaluation >= beta:\n",
    "                return evaluation, move\n",
    "            if evaluation > maxEvaluation:\n",
    "                best_move = move\n",
    "                maxEvaluation = evaluation\n",
    "        return maxEvaluation, best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `minValue` berechnet wie bisher die minimale Evaluierung aller möglichen Züge und enthält ansonsten dieselben Änderungen wie die Funktion `maxValue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentalAI(ExperimentalAI):  # type: ignore\n",
    "    def minValue(\n",
    "        self,\n",
    "        board: chess.Board,\n",
    "        current_evaluation: int,\n",
    "        depth: int,\n",
    "        level: int,\n",
    "        alpha: int,\n",
    "        beta: int,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        \"\"\"Searches the best value with given depth using minimax algorithm\"\"\"\n",
    "        self.stats[-1][\"nodes\"] += 1\n",
    "        early_abort_evaluation = self.minimax_early_abort(\n",
    "            board, level, depth, current_evaluation\n",
    "        )\n",
    "        if early_abort_evaluation is not None:\n",
    "            self.stats[-1][\"leaf_ctr\"] += 1\n",
    "            self.stats[-1][\"depth_sum\"] += level\n",
    "            return early_abort_evaluation, None\n",
    "\n",
    "        best_move = None\n",
    "        moves: list[tuple[int, int, chess.Move]] = []\n",
    "\n",
    "        # Black to play (negative numbers are good)\n",
    "        if level < self.DEPTH - 2:\n",
    "            for i, move in enumerate(board.legal_moves):\n",
    "                board.push(move)\n",
    "                key = self.get_key(board)\n",
    "                board.pop()\n",
    "                old_eval = self.cache_old.get(key, (None, i, None))[1]\n",
    "                heapq.heappush(moves, (old_eval, i, move))\n",
    "        else:\n",
    "            moves = list(enumerate(board.legal_moves))\n",
    "        minEvaluation = beta\n",
    "        while moves:\n",
    "            move = heapq.heappop(moves)[-1]\n",
    "            new_evaluation = self.incremental_evaluate(board, current_evaluation, move)\n",
    "            if depth > 1 or level + 1 == self.MAX_SVE_DEPTH:\n",
    "                new_depth = depth\n",
    "                board.push(move)\n",
    "            elif self.is_quiet_move_and_push(board, move):\n",
    "                new_depth = depth\n",
    "            else:\n",
    "                new_depth = depth + 1\n",
    "            evaluation, _ = self.minimax(\n",
    "                self.maxValue,\n",
    "                board,\n",
    "                new_evaluation,\n",
    "                new_depth - 1,\n",
    "                level + 1,\n",
    "                alpha,\n",
    "                minEvaluation,\n",
    "            )\n",
    "            board.pop()\n",
    "            if evaluation <= alpha:\n",
    "                return evaluation, move\n",
    "            if evaluation < minEvaluation:\n",
    "                best_move = move\n",
    "                minEvaluation = evaluation\n",
    "        return minEvaluation, best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Methode `store_in_cache` ist identisch zu der Implementierung in `Exercise06AI`. In der Methode `get_from_cache` existiert der minimale Unterschied, dass anstatt des `depth`-Parameters die Argumente `limit` und `level` durchgereicht werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentalAI(ExperimentalAI):  # type: ignore\n",
    "    def get_key(self, board: chess.Board) -> tuple:\n",
    "        \"\"\"Calculates a key that uniquely identifies a given board.\"\"\"\n",
    "        return (\n",
    "            board.pawns,\n",
    "            board.knights,\n",
    "            board.bishops,\n",
    "            board.rooks,\n",
    "            board.queens,\n",
    "            board.kings,\n",
    "            board.occupied_co[chess.WHITE],\n",
    "            board.occupied_co[chess.BLACK],\n",
    "            board.turn,\n",
    "            board.castling_rights,\n",
    "            board.halfmove_clock\n",
    "            if board.halfmove_clock >= (50 - self.MAX_SVE_DEPTH)\n",
    "            else -42,\n",
    "        )\n",
    "\n",
    "    def store_in_cache(\n",
    "        self, key: tuple, result: tuple, depth: int, alpha: int, beta: int\n",
    "    ) -> None:\n",
    "        \"\"\"Stores the result of a minimax computation in the cache.\"\"\"\n",
    "        evaluation, move = result\n",
    "        if evaluation <= alpha:\n",
    "            self.cache_new[key] = (\"≤\", evaluation, move, depth)\n",
    "        elif evaluation < beta:\n",
    "            self.cache_new[key] = (\"=\", evaluation, move, depth)\n",
    "        else:\n",
    "            self.cache_new[key] = (\"≥\", evaluation, move, depth)\n",
    "\n",
    "    def get_from_cache(\n",
    "        self,\n",
    "        minValue_or_maxValue: Callable,\n",
    "        key: tuple,\n",
    "        board: chess.Board,\n",
    "        current_eval: int,\n",
    "        depth: int,\n",
    "        level: int,\n",
    "        alpha: int,\n",
    "        beta: int,\n",
    "    ) -> tuple:\n",
    "        \"\"\"Gets a result from the cache if possible.\"\"\"\n",
    "        flag, evaluation, move, eval_depth = self.cache_new.get(\n",
    "            key\n",
    "        ) or self.cache_old.get(key)\n",
    "        if depth > eval_depth:\n",
    "            result = minValue_or_maxValue(\n",
    "                board, current_eval, depth, level, alpha, beta\n",
    "            )\n",
    "            self.store_in_cache(key, result, depth, alpha, beta)\n",
    "            return result\n",
    "        if flag == \"=\":\n",
    "            return evaluation, move\n",
    "        elif flag == \"≤\":\n",
    "            if evaluation <= alpha:\n",
    "                return evaluation, move\n",
    "            elif evaluation < beta:\n",
    "                result = minValue_or_maxValue(\n",
    "                    board, current_eval, depth, level, alpha, evaluation\n",
    "                )\n",
    "                self.store_in_cache(key, result, depth, alpha, evaluation)\n",
    "                return result\n",
    "            else:\n",
    "                result = minValue_or_maxValue(\n",
    "                    board, current_eval, depth, level, alpha, beta\n",
    "                )\n",
    "                self.store_in_cache(key, result, depth, alpha, beta)\n",
    "                return result\n",
    "        else:\n",
    "            if evaluation <= alpha:\n",
    "                result = minValue_or_maxValue(\n",
    "                    board, current_eval, depth, level, alpha, beta\n",
    "                )\n",
    "                self.store_in_cache(key, result, depth, alpha, beta)\n",
    "                return result\n",
    "            elif evaluation < beta:\n",
    "                result = minValue_or_maxValue(\n",
    "                    board, current_eval, depth, level, evaluation, beta\n",
    "                )\n",
    "                self.store_in_cache(key, result, depth, evaluation, beta)\n",
    "                return result\n",
    "            else:\n",
    "                return evaluation, move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Methode `evaluate_minimax` ist größtenteils identisch zu der bisherigen `memoize_minimax` Funktion.  \n",
    "Folgende Änderungen wurden eingeführt:\n",
    "- Die Funktion wird nicht mehr als Dekorator implementiert sondern direkt aufgerufen.\n",
    "- Da die Funktion `minimax` in zwei separate Funktionen aufgeteilt wurde, gibt es nun einen neuen Parameter `minimax`, welcher eine Referenz der Funktion `minValue` oder `maxValue` enthält.\n",
    "- Statt mit dem Parameter `depth` wird die `get_key` Funktion intern mit der Differenz aus `limit` und `level` aufgerufen. Die Differenz ist dabei das zum aktuellen Board relative Limit, also mit welcher Tiefe die Suche vom aktuellen Board ausgehend durchgeführt werden soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ExperimentalAI(ExperimentalAI):  # type: ignore\n",
    "    # @debug_minimax\n",
    "    def minimax(\n",
    "        self,\n",
    "        minValue_or_maxValue: Callable,\n",
    "        board: chess.Board,\n",
    "        current_evaluation: int,\n",
    "        depth: int,\n",
    "        level: int = 0,\n",
    "        alpha: int = -ExperimentalAI.LIMIT,\n",
    "        beta: int = ExperimentalAI.LIMIT,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        \"\"\"Searches the best value with given depth using minimax algorithm\"\"\"\n",
    "        key = self.get_key(board)\n",
    "        self.stats[-1][\"cache_tries\"] += 1\n",
    "        self.stats[-1][\"max_depth\"] = max(level, self.stats[-1][\"max_depth\"])\n",
    "\n",
    "        if key in self.cache_new or key in self.cache_old:\n",
    "            self.stats[-1][\"cache_hits\"] += 1\n",
    "            return self.get_from_cache(\n",
    "                minValue_or_maxValue,\n",
    "                key,\n",
    "                board,\n",
    "                current_evaluation,\n",
    "                depth,\n",
    "                level,\n",
    "                alpha,\n",
    "                beta,\n",
    "            )\n",
    "        result = minValue_or_maxValue(\n",
    "            board, current_evaluation, depth, level, alpha, beta\n",
    "        )\n",
    "        self.store_in_cache(key, result, depth, alpha, beta)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Methode `get_next_middle_game_move` wird nun bei jedem Zug überprüft, welcher Spieler an der Reihe ist. Abhängig davon wird `evaluate_minimax` entweder mit `minValue` oder mit `maxValue` aufgerufen, um den besten Zug zu ermitteln. Alle sonstigen Änderungen dienen nur der genaueren Erfassung von statistischen Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class ExperimentalAI(ExperimentalAI):  # type: ignore\n",
    "    def get_next_middle_game_move(self, board: chess.Board) -> chess.Move:\n",
    "        \"\"\"Gets the best next move\"\"\"\n",
    "        self.last_evaluation: int | None  # type annotation for mypy\n",
    "        self.stats[-1][\"cache_hits\"] = 0\n",
    "        self.stats[-1][\"cache_tries\"] = 0\n",
    "        self.stats[-1][\"leaf_ctr\"] = 0\n",
    "        self.stats[-1][\"max_depth\"] = -1\n",
    "        self.stats[-1][\"avg_depth\"] = -1\n",
    "        self.stats[-1][\"depth_sum\"] = 0\n",
    "\n",
    "        if self.is_king_endgame != self.check_king_endgame(board):\n",
    "            self.last_evaluation += self.get_endgame_evaluation_change(board)\n",
    "        # Calculate current evaluation\n",
    "        if self.last_evaluation is None:  # type: ignore\n",
    "            current_evaluation = self.full_evaluate(board)\n",
    "        else:\n",
    "            # Get current evaluation (after opponent move)\n",
    "            last_move = board.pop()\n",
    "            current_evaluation = self.incremental_evaluate(board, self.last_evaluation, last_move)  # type: ignore\n",
    "            board.push(last_move)\n",
    "\n",
    "        evaluation_function = self.maxValue if board.turn else self.minValue\n",
    "        self.stats[-1][\"nodes\"] = 0\n",
    "        future_evaluation, best_move = self.minimax(\n",
    "            evaluation_function, board, current_evaluation, self.DEPTH\n",
    "        )\n",
    "\n",
    "        # Debugging fail safe\n",
    "        assert best_move, f\"\"\"\n",
    "        Best move is None with fen '{board.fen()}' at player {type(self).__name__}! \n",
    "        depth: {self.DEPTH}, last_eval: {self.last_evaluation}, current_evaluation: {current_evaluation},\n",
    "        is_king_engame: {getattr(self, 'is_king_endgame', \"N/A\")}, move_stack: {board.move_stack}\n",
    "        \"\"\"\n",
    "        # Update last evaluation (after player move)\n",
    "        self.last_evaluation = self.incremental_evaluate(\n",
    "            board, current_evaluation, best_move\n",
    "        )\n",
    "        # Update stats\n",
    "        self.stats[-1][\"minimax_eval\"] = future_evaluation\n",
    "        self.stats[-1][\"board_eval_before_move\"] = current_evaluation\n",
    "        self.stats[-1][\"board_eval_after_move\"] = self.last_evaluation\n",
    "        self.stats[-1][\"avg_depth\"] = self.stats[-1][\"depth_sum\"] / (\n",
    "            self.stats[-1][\"leaf_ctr\"] or 1\n",
    "        )\n",
    "        del self.stats[-1][\"depth_sum\"]\n",
    "        del self.stats[-1][\"leaf_ctr\"]\n",
    "        self.stats[-1][\"cache_size_mb\"] = round(\n",
    "            (sys.getsizeof(self.cache_new) + sys.getsizeof(self.cache_old))\n",
    "            / (1024 * 1024),\n",
    "            2,\n",
    "        )\n",
    "        self.cache_old.clear()\n",
    "        self.cache_old = self.cache_new\n",
    "        self.cache_new = {}\n",
    "\n",
    "        self.stats[-1][\"datetime\"] = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        return best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Bereich\n",
    "\n",
    "Die folgenden Zellen enthalten Unit-Tests der oben implementierten Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Exercise02AI as Exercise02AI_\n",
    "import Exercise04AI as Exercise04AI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create player and board\n",
    "unit_test_player = ExperimentalAI(player_name=\"ExpAI\", search_depth=3, max_depth=3)\n",
    "board = chess.Board(\"5rk1/1b3p2/8/3p4/3p2P1/2Q4B/5P1K/R3R3 b - - 0 36\")\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test minimax\n",
    "def test_minimax(\n",
    "    unit_test_player: ChessAI,\n",
    "    board: chess.Board,\n",
    "    current_evaluation: int,\n",
    "    expected_evaluation: int,\n",
    "    expected_move: str,\n",
    "    expected_nodes: int,\n",
    "    expected_cache_tries: int,\n",
    "    expected_cache_hits: int,\n",
    "    expected_cache_elements: int,\n",
    "):\n",
    "    unit_test_player.cache_new = {}  # Clear cache\n",
    "    unit_test_player.cache_old = {}  # Clear cache\n",
    "    unit_test_player.stats[-1][\"cache_tries\"] = 0\n",
    "    unit_test_player.stats[-1][\"cache_hits\"] = 0\n",
    "    unit_test_player.stats[-1][\"nodes\"] = 0\n",
    "    unit_test_player.stats[-1][\"leaf_ctr\"] = 0\n",
    "    unit_test_player.stats[-1][\"max_depth\"] = 0\n",
    "    unit_test_player.stats[-1][\"depth_sum\"] = 0\n",
    "    f = unit_test_player.maxValue if board.turn else unit_test_player.minValue\n",
    "    mm_evaluation, mm_move = unit_test_player.minimax(\n",
    "        f, board, current_evaluation, unit_test_player.DEPTH\n",
    "    )\n",
    "    nodes = unit_test_player.stats[-1][\"nodes\"]\n",
    "    cache_tries = unit_test_player.stats[-1]['cache_tries']\n",
    "    cache_hits = unit_test_player.stats[-1]['cache_hits']\n",
    "    print(f\"Minimax Evaluation: {mm_evaluation}\")\n",
    "    print(f\"Minimax Move: {mm_move}\")\n",
    "    print(f\"Nodes searched: {nodes}\")\n",
    "    print(f\"Cache tries: {cache_tries}\")\n",
    "    print(f\"Cache hits: {cache_hits}\")\n",
    "    print(f\"Elements in new cache: {len(unit_test_player.cache_new)}\")\n",
    "    print(f\"Elements in old cache: {len(unit_test_player.cache_old)}\")\n",
    "    assert (\n",
    "        mm_evaluation == expected_evaluation\n",
    "    ), \"Minimax evaluation does not match expected value!\"\n",
    "    assert mm_move.uci() == expected_move, \"Minimax move does not match expected value!\"\n",
    "    assert nodes == expected_nodes, \"Searched node count has changed!\"\n",
    "    assert cache_tries == expected_cache_tries, \"Cache tries do not match expected value!\"\n",
    "    assert cache_hits == expected_cache_hits, \"Cache hits do not match expected value!\"\n",
    "    assert len(unit_test_player.cache_new) == expected_cache_elements, \"Cache elements do not match expected value!\"\n",
    "    assert len(unit_test_player.cache_old) == 0, \"Cache elements do not match expected value!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_minimax(\n",
    "    unit_test_player,\n",
    "    board,\n",
    "    current_evaluation=1240,\n",
    "    expected_evaluation=325,\n",
    "    expected_move=\"d4c3\",\n",
    "    expected_nodes=2336,\n",
    "    expected_cache_tries=2788,\n",
    "    expected_cache_hits=557,\n",
    "    expected_cache_elements=2231,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test next move function (with memoized minimax result)\n",
    "Exercise02AI_.test_next_move(\n",
    "    unit_test_player,\n",
    "    board,\n",
    "    expected_move=\"d4c3\",\n",
    "    expected_nodes=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporärer Bereich\n",
    "\n",
    "Der folgende Bereich dient zum temporären Debuggen und kann nicht-funktionierenden Code enthalten. Dieser Bereich wird vor der Abgabe entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
