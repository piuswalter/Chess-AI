{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".container {\n",
    "  width: 100%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from AIBaseClass import ChessAI\n",
    "from Exercise03AI import Exercise03AI\n",
    "from Exercise04AI import Exercise04AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 08: Minimax mit Alpha-Beta-Pruning, Memoisierung, Progressive Deepening und Singular Value Extension\n",
    "\n",
    "Dieses Notebook erweitert den Minimax-Algorithmus um die Singular Value Extension. Hierbei wird die Suchtiefe im jeweiligen Teilbaum um 1 erhöht, wenn entweder eine Figur geschlagen wurde, der König im Schach steht oder eine Figurenumwandlung durchgeführt wurde. Die Singular Value Extension (SVE) reduziert den sogenannten Horizont-Effekt, d.h. eine gute Einschätzung eines Boards am Tiefenlimit, wobei es aber einen nächsten, nicht-untersuchten Zug gibt, mit welchem sich die Stellung stark verschlechtert. Die Erhöhung wird dabei nur bis zu einem definierten Limit (`MAX_SVE_DEPTH`) durchgeführt, da ansonsten zu viele Ressourcen benötigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "from typing import Any, Callable\n",
    "\n",
    "\n",
    "class Exercise08AI(Exercise03AI):\n",
    "    \"\"\"Chooses middle game moves using minimax algorithm, alpha-beta-pruning,\n",
    "    memoization, progressive deepening and the singular value extension\"\"\"\n",
    "\n",
    "    def __init__(self, max_depth: int = 8, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.cache: dict[tuple, tuple[str, int, chess.Move]] = {}\n",
    "        self.MAX_SVE_DEPTH: int = max(max_depth, kwargs[\"search_depth\"])\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resets all internal variables\"\"\"\n",
    "        super().reset()\n",
    "        self.cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `debug_minimax` ist eine Debugging-Hilfsfunktion um die Berechnungen der `evaluate_minimax`-Funktion nachvollziehen zu können. Die Funktion wird bei Bedarf als Dekorator auf die Funktion `evaluate_minimax` angewendet, ist aber standardmäßig nicht aktiviert. Die Parameter sind dieselben wie bei der Funktion `evaluate_minimax` und werden im dortigen Abschnitt beschrieben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_minimax(minimax: Callable):\n",
    "    \"\"\"Prints the Minimax Game Tree.\"\"\"\n",
    "\n",
    "    def minimax_debug(\n",
    "        self,\n",
    "        minValue_or_maxValue: Callable,\n",
    "        board: chess.Board,\n",
    "        current_evaluation: int,\n",
    "        depth: int,\n",
    "        level: int = 0,\n",
    "        alpha: int = -Exercise08AI.LIMIT,\n",
    "        beta: int = Exercise08AI.LIMIT,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        color = \"white\" if board.turn else \"black\"\n",
    "        ws = (level + 1) * \"    \"\n",
    "        key = Exercise04AI.get_key(board, depth)\n",
    "        if key in self.cache:\n",
    "            print(\n",
    "                f\"level: {level} {ws} mm> {color}; α: {alpha}; β: {beta}; depth: {depth}\"\n",
    "            )  # memoization\n",
    "        else:\n",
    "            print(\n",
    "                f\"level: {level} {ws} --> {color}; α: {alpha}; β: {beta}; depth: {depth}\"\n",
    "            )\n",
    "        evaluation, best_move = minimax(\n",
    "            self,\n",
    "            minValue_or_maxValue,\n",
    "            board,\n",
    "            current_evaluation,\n",
    "            depth,\n",
    "            level,\n",
    "            alpha,\n",
    "            beta,\n",
    "        )\n",
    "        print(\n",
    "            f\"level: {level} {ws} <-- {color}; ev: {evaluation}; best_move: {best_move}\"\n",
    "        )\n",
    "        return evaluation, best_move\n",
    "\n",
    "    return minimax_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `is_quiet_move_and_push` ist ein Indikator dafür, ob die Suchtiefe (`depth`) erhöht werden muss. Dabei prüft die Funktion ob der übergebene Zug ein Schlagzug, eine Umwandlung oder ein Schachzug ist und gibt entsprechend `True` bzw. `False` zurück. Der übergebene Zug wird im Laufe der Prüfung auf das Board angewendet.\n",
    "\n",
    "Hinweis: Die `chess`-Bibliothek bietet auch die Funktion `gives_check` an, welche überprüft ob ein übergebener Zug den Gegner in Schach setzt. Da diese Funktion aber intern nur den Zug anwendet, `is_check` aufruft und den Zug anschließend wieder entfernt wurde aus Effizienzgründen direkt letztere Funktion verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def is_quiet_move_and_push(self, board: chess.Board, move: chess.Move) -> bool:\n",
    "        \"\"\"Checks if the next move was an promotion, capture or check move. Pushes the given move on the board.\"\"\"\n",
    "        if move.promotion or board.piece_type_at(move.to_square):\n",
    "            board.push(move)\n",
    "            return False\n",
    "        board.push(move)\n",
    "        if board.is_check():\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Singular Value Extension erfordert es, dass die Suchtiefe für die Suche nun erstmals laufend angepasst werden muss. Um eine statistische Erfassung der Suchtiefe zu ermöglichen und für effizientere Überprüfungen wird der Parameter `depth`, nun durch den Parameter `level` ergänzt. Das Argument `level` ist nun die aktuelle Suchtiefe, welche bei `0` startet und mit jedem Aufruf von `minValue` oder `maxValue` inkrementiert wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def minimax_early_abort(\n",
    "        self, board: chess.Board, level: int, depth: int, current_evaluation: int\n",
    "    ) -> int | None:\n",
    "        \"\"\"Returns an evaluation iff the minimax has an early exit condition. Returns None otherwise.\"\"\"\n",
    "        is_checkmate = board.is_checkmate()\n",
    "        if is_checkmate and not board.turn:\n",
    "            # White has won the game\n",
    "            evaluation = self.LIMIT - level\n",
    "            return evaluation\n",
    "\n",
    "        if is_checkmate and board.turn:\n",
    "            # Black has won the game\n",
    "            evaluation = -self.LIMIT + level\n",
    "            return evaluation\n",
    "\n",
    "        if (\n",
    "            board.is_insufficient_material()\n",
    "            or not board.legal_moves\n",
    "            or board.is_fifty_moves()\n",
    "        ):\n",
    "            # Game is a draw\n",
    "            return 0\n",
    "\n",
    "        # Recursion abort case\n",
    "        if depth == 0:\n",
    "            return current_evaluation\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Lesbarkeit der Minimax Funktion zu wahren, wurde diese in die Funktionen `maxValue`, `minValue` und `evaluate_minimax` aufgeteilt, wobei `maxValue` und `minValue` die beiden Zweige des Minimax Algorithmus darstellen.\n",
    "Die `evaluate_minimax` übernimmt nun die Memoisierung, wodurch kein Dekorator mehr benötigt wird.\n",
    "\n",
    "Die Funtion `maxValue` berechnet wie bisher die maximale Evaluierung aller möglichen Züge. Neu hinzugekommen ist die laufende Erhöhung der Suchtiefe, welche direkt nach dem Entfernen eines Zuges von der Prioritätswarteschlange `moves` durchgeführt wird. Dort werden nun drei Fälle unterschieden:\n",
    "- Wenn die Ungleichung `depth > 1` erfüllt ist, wird die Suchtiefe grundsätzlich nicht erhöht, ebenso wenn die maximale Tiefe für die SVE erreicht ist. Dies hat zur Folge, dass die SVE erst zum Einsatz kommt, wenn die Zielsuchtiefe im nächsten Schritt erreicht ist und somit die Suche sonst zu Ende wäre. Diese Bedingung wurde aus Performance-Gründen mit aufgenommen, da die Berechnungen ansonsten durch die teilweise sehr hohe Tiefe wesentlich länger dauern. Des weiteren vereinfacht es den Zugriff auf den Cache im Progressive Deepening (die Berechnung der Tiefe des letzten Durchlaufes).\n",
    "- Wenn der Zug \"ruhig\" ist, d.h. es sich nicht um einen Schlagzug, eine Umwandlung oder ein Schachzug handelt, wird die Suchtiefe nicht erhöht.\n",
    "- Falls die ersten beiden Bedingungen nicht zutreffen, wird die SVE durchgeführt und die Suchtiefe um `1` erhöht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "\n",
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def maxValue(\n",
    "        self,\n",
    "        board: chess.Board,\n",
    "        current_evaluation: int,\n",
    "        depth: int,\n",
    "        level: int,\n",
    "        alpha: int,\n",
    "        beta: int,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        \"\"\"Searches the best value with given depth using minimax algorithm\"\"\"\n",
    "        self.stats[-1][\"nodes\"] += 1\n",
    "        early_abort_evaluation = self.minimax_early_abort(\n",
    "            board, level, depth, current_evaluation\n",
    "        )\n",
    "        if early_abort_evaluation is not None:\n",
    "            self.stats[-1][\"leaf_ctr\"] += 1\n",
    "            self.stats[-1][\"depth_sum\"] += level  # for calculating the average depth\n",
    "            return early_abort_evaluation, None\n",
    "\n",
    "        best_move = None\n",
    "        moves: list[tuple[int, int, chess.Move]] = []\n",
    "\n",
    "        # White to play (positive numbers are good)\n",
    "        for i, move in enumerate(board.legal_moves):\n",
    "            board.push(move)\n",
    "            key = Exercise04AI.get_key(board, depth - 2)\n",
    "            board.pop()\n",
    "            old_eval = self.cache.get(key, (None, i, None))[1]\n",
    "            heapq.heappush(moves, (-old_eval, i, move))\n",
    "        maxEvaluation = alpha\n",
    "        while moves:\n",
    "            move = heapq.heappop(moves)[2]\n",
    "            new_evaluation = self.incremental_evaluate(board, current_evaluation, move)\n",
    "            if depth > 1 or level + 1 == self.MAX_SVE_DEPTH:\n",
    "                new_depth = depth\n",
    "                board.push(move)\n",
    "            elif self.is_quiet_move_and_push(board, move):\n",
    "                new_depth = depth\n",
    "            else:\n",
    "                new_depth = depth + 1\n",
    "            evaluation, _, = self.minimax(\n",
    "                self.minValue,\n",
    "                board,\n",
    "                new_evaluation,\n",
    "                new_depth - 1,\n",
    "                level + 1,\n",
    "                maxEvaluation,\n",
    "                beta,\n",
    "            )\n",
    "            board.pop()\n",
    "            if evaluation >= beta:\n",
    "                return evaluation, move\n",
    "            if evaluation > maxEvaluation:\n",
    "                best_move = move\n",
    "                maxEvaluation = evaluation\n",
    "        return maxEvaluation, best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `minValue` berechnet wie bisher die minimale Evaluierung aller möglichen Züge und enthält ansonsten dieselben Änderungen wie die Funktion `maxValue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def minValue(\n",
    "        self,\n",
    "        board: chess.Board,\n",
    "        current_evaluation: int,\n",
    "        depth: int,\n",
    "        level: int,\n",
    "        alpha: int,\n",
    "        beta: int,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        \"\"\"Searches the best value with given depth using minimax algorithm\"\"\"\n",
    "        self.stats[-1][\"nodes\"] += 1\n",
    "        early_abort_evaluation = self.minimax_early_abort(\n",
    "            board, level, depth, current_evaluation\n",
    "        )\n",
    "        if early_abort_evaluation is not None:\n",
    "            self.stats[-1][\"leaf_ctr\"] += 1\n",
    "            self.stats[-1][\"depth_sum\"] += level\n",
    "            return early_abort_evaluation, None\n",
    "\n",
    "        best_move = None\n",
    "        moves: list[tuple[int, int, chess.Move]] = []\n",
    "\n",
    "        # Black to play (negative numbers are good)\n",
    "        for i, move in enumerate(board.legal_moves):\n",
    "            board.push(move)\n",
    "            key = Exercise04AI.get_key(board, depth - 2)\n",
    "            board.pop()\n",
    "            old_eval = self.cache.get(key, (None, i, None))[1]\n",
    "            heapq.heappush(moves, (old_eval, i, move))\n",
    "        minEvaluation = beta\n",
    "        while moves:\n",
    "            move = heapq.heappop(moves)[2]\n",
    "            new_evaluation = self.incremental_evaluate(board, current_evaluation, move)\n",
    "            if depth > 1 or level + 1 == self.MAX_SVE_DEPTH:\n",
    "                new_depth = depth\n",
    "                board.push(move)\n",
    "            elif self.is_quiet_move_and_push(board, move):\n",
    "                new_depth = depth\n",
    "            else:\n",
    "                new_depth = depth + 1\n",
    "            evaluation, _ = self.minimax(\n",
    "                self.maxValue,\n",
    "                board,\n",
    "                new_evaluation,\n",
    "                new_depth - 1,\n",
    "                level + 1,\n",
    "                alpha,\n",
    "                minEvaluation,\n",
    "            )\n",
    "            board.pop()\n",
    "            if evaluation <= alpha:\n",
    "                return evaluation, move\n",
    "            if evaluation < minEvaluation:\n",
    "                best_move = move\n",
    "                minEvaluation = evaluation\n",
    "        return minEvaluation, best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Methode `store_in_cache` ist identisch zu der Implementierung in `Exercise06AI`. In der Methode `get_from_cache` existiert der minimale Unterschied, dass anstatt des `depth`-Parameters zusätzlich das Argument `level` durchgereicht wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def store_in_cache(self, key: tuple, result: tuple, alpha: int, beta: int) -> None:\n",
    "        \"\"\"Stores the result of a minimax computation in the cache.\"\"\"\n",
    "        evaluation, move = result\n",
    "        if evaluation <= alpha:\n",
    "            self.cache[key] = (\"≤\", evaluation, move)\n",
    "        elif evaluation < beta:\n",
    "            self.cache[key] = (\"=\", evaluation, move)\n",
    "        else:\n",
    "            self.cache[key] = (\"≥\", evaluation, move)\n",
    "\n",
    "    def get_from_cache(\n",
    "        self,\n",
    "        minValue_or_maxValue: Callable,\n",
    "        key: tuple,\n",
    "        board: chess.Board,\n",
    "        current_eval: int,\n",
    "        depth: int,\n",
    "        level: int,\n",
    "        alpha: int,\n",
    "        beta: int,\n",
    "    ) -> tuple:\n",
    "        \"\"\"Gets a result from the cache if possible.\"\"\"\n",
    "        flag, evaluation, move = self.cache[key]\n",
    "        if flag == \"=\":\n",
    "            return evaluation, move\n",
    "        elif flag == \"≤\":\n",
    "            if evaluation <= alpha:\n",
    "                return evaluation, move\n",
    "            elif evaluation < beta:\n",
    "                result = minValue_or_maxValue(\n",
    "                    board, current_eval, depth, level, alpha, evaluation\n",
    "                )\n",
    "                self.store_in_cache(key, result, alpha, evaluation)\n",
    "                return result\n",
    "            else:\n",
    "                result = minValue_or_maxValue(\n",
    "                    board, current_eval, depth, level, alpha, beta\n",
    "                )\n",
    "                self.store_in_cache(key, result, alpha, beta)\n",
    "                return result\n",
    "        else:\n",
    "            if evaluation <= alpha:\n",
    "                result = minValue_or_maxValue(\n",
    "                    board, current_eval, depth, level, alpha, beta\n",
    "                )\n",
    "                self.store_in_cache(key, result, alpha, beta)\n",
    "                return result\n",
    "            elif evaluation < beta:\n",
    "                result = minValue_or_maxValue(\n",
    "                    board, current_eval, depth, level, evaluation, beta\n",
    "                )\n",
    "                self.store_in_cache(key, result, evaluation, beta)\n",
    "                return result\n",
    "            else:\n",
    "                return evaluation, move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Methode `evaluate_minimax` ist größtenteils identisch zu der bisherigen `memoize_minimax` Funktion.  \n",
    "Folgende Änderungen wurden eingeführt:\n",
    "- Die Funktion wird nicht mehr als Dekorator implementiert sondern direkt aufgerufen.\n",
    "- Da die Funktion `minimax` in zwei separate Funktionen aufgeteilt wurde, gibt es nun einen neuen Parameter `minValue_or_maxValue`, welcher eine Referenz der Funktion `minValue` oder `maxValue` enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    # @debug_minimax\n",
    "    def minimax(\n",
    "        self,\n",
    "        minValue_or_maxValue: Callable,\n",
    "        board: chess.Board,\n",
    "        current_evaluation: int,\n",
    "        depth: int,\n",
    "        level: int = 0,\n",
    "        alpha: int = -Exercise08AI.LIMIT,\n",
    "        beta: int = Exercise08AI.LIMIT,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        \"\"\"Searches the best value with given depth using minimax algorithm\"\"\"\n",
    "        key = Exercise04AI.get_key(board, depth)\n",
    "        self.stats[-1][\"cache_tries\"] += 1\n",
    "        self.stats[-1][\"max_depth\"] = max(level, self.stats[-1][\"max_depth\"])\n",
    "\n",
    "        if key in self.cache:\n",
    "            self.stats[-1][\"cache_hits\"] += 1\n",
    "            return self.get_from_cache(\n",
    "                minValue_or_maxValue,\n",
    "                key,\n",
    "                board,\n",
    "                current_evaluation,\n",
    "                depth,\n",
    "                level,\n",
    "                alpha,\n",
    "                beta,\n",
    "            )\n",
    "        result = minValue_or_maxValue(\n",
    "            board, current_evaluation, depth, level, alpha, beta\n",
    "        )\n",
    "        self.store_in_cache(key, result, alpha, beta)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Methode `get_next_middle_game_move` wird nun bei jedem Zug überprüft, welcher Spieler an der Reihe ist. Abhängig davon wird `evaluate_minimax` entweder mit `minValue` oder mit `maxValue` aufgerufen, um den besten Zug zu ermitteln. Alle sonstigen Änderungen dienen nur der genaueren Erfassung von statistischen Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def get_next_middle_game_move(self, board: chess.Board) -> chess.Move:\n",
    "        \"\"\"Gets the best next move\"\"\"\n",
    "        self.last_evaluation: int | None  # type annotation for mypy\n",
    "        self.stats[-1][\"cache_hits\"] = 0\n",
    "        self.stats[-1][\"cache_tries\"] = 0\n",
    "        self.stats[-1][\"leaf_ctr\"] = 0\n",
    "        self.stats[-1][\"max_depth\"] = -1\n",
    "        self.stats[-1][\"avg_depth\"] = -1\n",
    "        self.stats[-1][\"depth_sum\"] = 0\n",
    "\n",
    "        if self.is_king_endgame != self.check_king_endgame(board):\n",
    "            self.last_evaluation += self.get_endgame_evaluation_change(board)\n",
    "        # Calculate current evaluation\n",
    "        if self.last_evaluation is None:  # type: ignore\n",
    "            current_evaluation = self.full_evaluate(board)\n",
    "        else:\n",
    "            # Get current evaluation (after opponent move)\n",
    "            last_move = board.pop()\n",
    "            current_evaluation = self.incremental_evaluate(board, self.last_evaluation, last_move)  # type: ignore\n",
    "            board.push(last_move)\n",
    "\n",
    "        evaluation_function = self.maxValue if board.turn else self.minValue\n",
    "        self.stats[-1][\"nodes\"] = 0\n",
    "        for depth in range(1, self.DEPTH + 1):\n",
    "            # Call minimax and get best move\n",
    "            future_evaluation, best_move = self.minimax(\n",
    "                evaluation_function, board, current_evaluation, depth\n",
    "            )\n",
    "\n",
    "        # Debugging fail safe\n",
    "        assert best_move, f\"\"\"\n",
    "        Best move is None with fen '{board.fen()}' at player {type(self).__name__}! \n",
    "        depth: {self.DEPTH}, last_eval: {self.last_evaluation}, current_evaluation: {current_evaluation},\n",
    "        is_king_engame: {getattr(self, 'is_king_endgame', \"N/A\")}, move_stack: {board.move_stack}\n",
    "        \"\"\"\n",
    "        # Update last evaluation (after player move)\n",
    "        self.last_evaluation = self.incremental_evaluate(\n",
    "            board, current_evaluation, best_move\n",
    "        )\n",
    "        # Update stats\n",
    "        self.stats[-1][\"minimax_eval\"] = future_evaluation\n",
    "        self.stats[-1][\"board_eval_before_move\"] = current_evaluation\n",
    "        self.stats[-1][\"board_eval_after_move\"] = self.last_evaluation\n",
    "        self.stats[-1][\"avg_depth\"] = self.stats[-1][\"depth_sum\"] / (\n",
    "            self.stats[-1][\"leaf_ctr\"] or 1\n",
    "        )\n",
    "        del self.stats[-1][\"depth_sum\"]\n",
    "        del self.stats[-1][\"leaf_ctr\"]\n",
    "        self.stats[-1][\"cache_size_mb\"] = round(\n",
    "            sys.getsizeof(self.cache) / (1024 * 1024), 2\n",
    "        )\n",
    "        if board.is_irreversible(best_move):\n",
    "            self.cache.clear()\n",
    "            self.stats[-1][\"cache_cleared\"] = True\n",
    "        else:\n",
    "            self.stats[-1][\"cache_cleared\"] = False\n",
    "        self.stats[-1][\"datetime\"] = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        return best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Bereich\n",
    "\n",
    "Die folgenden Zellen enthalten Unit-Tests der oben implementierten Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Exercise02AI as Exercise02AI_\n",
    "import Exercise04AI as Exercise04AI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create player and board\n",
    "unit_test_player = Exercise08AI(player_name=\"Ex08AI\", search_depth=3, max_depth=3)\n",
    "board = chess.Board(\"5rk1/1b3p2/8/3p4/3p2P1/2Q4B/5P1K/R3R3 b - - 0 36\")\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test minimax\n",
    "def test_minimax(\n",
    "    unit_test_player: ChessAI,\n",
    "    board: chess.Board,\n",
    "    current_evaluation: int,\n",
    "    expected_evaluation: int,\n",
    "    expected_move: str,\n",
    "    expected_nodes: int,\n",
    "    expected_cache_tries: int,\n",
    "    expected_cache_hits: int,\n",
    "    expected_cache_elements: int,\n",
    "):\n",
    "    unit_test_player.cache = {}  # Clear cache\n",
    "    unit_test_player.stats[-1][\"cache_tries\"] = 0\n",
    "    unit_test_player.stats[-1][\"cache_hits\"] = 0\n",
    "    unit_test_player.stats[-1][\"nodes\"] = 0\n",
    "    unit_test_player.stats[-1][\"leaf_ctr\"] = 0\n",
    "    unit_test_player.stats[-1][\"max_depth\"] = 0\n",
    "    unit_test_player.stats[-1][\"depth_sum\"] = 0\n",
    "    f = unit_test_player.maxValue if board.turn else unit_test_player.minValue\n",
    "    mm_evaluation, mm_move = unit_test_player.minimax(\n",
    "        f, board, current_evaluation, unit_test_player.DEPTH\n",
    "    )\n",
    "    nodes = unit_test_player.stats[-1][\"nodes\"]\n",
    "    cache_tries = unit_test_player.stats[-1]['cache_tries']\n",
    "    cache_hits = unit_test_player.stats[-1]['cache_hits']\n",
    "    print(f\"Minimax Evaluation: {mm_evaluation}\")\n",
    "    print(f\"Minimax Move: {mm_move}\")\n",
    "    print(f\"Nodes searched: {nodes}\")\n",
    "    print(f\"Cache tries: {cache_tries}\")\n",
    "    print(f\"Cache hits: {cache_hits}\")\n",
    "    print(f\"Elements in cache: {len(unit_test_player.cache)}\")\n",
    "    assert (\n",
    "        mm_evaluation == expected_evaluation\n",
    "    ), \"Minimax evaluation does not match expected value!\"\n",
    "    assert mm_move.uci() == expected_move, \"Minimax move does not match expected value!\"\n",
    "    assert nodes == expected_nodes, \"Searched node count has changed!\"\n",
    "    assert cache_tries == expected_cache_tries, \"Cache tries do not match expected value!\"\n",
    "    assert cache_hits == expected_cache_hits, \"Cache hits do not match expected value!\"\n",
    "    assert len(unit_test_player.cache) == expected_cache_elements, \"Cache elements do not match expected value!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_minimax(\n",
    "    unit_test_player,\n",
    "    board,\n",
    "    current_evaluation=1240,\n",
    "    expected_evaluation=325,\n",
    "    expected_move=\"d4c3\",\n",
    "    expected_nodes=5553,\n",
    "    expected_cache_tries=7487,\n",
    "    expected_cache_hits=2560,\n",
    "    expected_cache_elements=4927,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test next move function (with memoized minimax result)\n",
    "Exercise02AI_.test_next_move(\n",
    "    unit_test_player,\n",
    "    board,\n",
    "    expected_move=\"d4c3\",\n",
    "    expected_nodes=80,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporärer Bereich\n",
    "\n",
    "Der folgende Bereich dient zum temporären Debuggen und kann nicht-funktionierenden Code enthalten. Dieser Bereich wird vor der Abgabe entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
