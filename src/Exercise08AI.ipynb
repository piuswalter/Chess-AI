{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".container {\n",
    "  width: 100%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from AIBaseClass import ChessAI\n",
    "from Exercise03AI import Exercise03AI\n",
    "from Exercise04AI import Exercise04AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 08: Minimax mit Alpha-Beta-Pruning, Memoisierung, Progressive Deepening und Singular Value Extension\n",
    "\n",
    "Dieses Notebook erweitert den Minimax-Algorithmus um die Singular Value Extension. Hierbei wird die Suchtiefe im jeweiligen Teilbaum um 1 erhöht, wenn entweder eine Figur geschlagen wurde, der König im Schach steht oder eine Figurenumwandlung durchgeführt wurde. Die Erhöhung wird dabei allerdings nur bis zu einem definierten Tiefenlimit (`MAX_SVE_DEPTH`) durchgeführt da sie ansonsten zu viel Ressourcen benötigt. Die Singular Value Extension reduziert den sog. Horizont-Effekt, d.h. eine gute Einschätzung eines Boards am Tiefenlimit wobei es aber einen nächsten Zug gibt, mit welchem sich die Stellung stark verschlechtert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "from typing import Any, Callable\n",
    "\n",
    "\n",
    "class Exercise08AI(Exercise03AI):\n",
    "    \"\"\"Chooses middle game moves using minimax algorithm, alpha-beta-pruning,\n",
    "    memoization and progressive deepening and the quiescence search\"\"\"\n",
    "\n",
    "    def __init__(self, max_depth: int = 10, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.cache: dict[tuple, Any] = {}\n",
    "        assert (\n",
    "            self.DEPTH < max_depth\n",
    "        ), \"Max depth needs to be greater than search depth!\"\n",
    "        self.MAX_SVE_DEPTH: int = max_depth\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resets all internal variables\"\"\"\n",
    "        super().reset()\n",
    "        self.cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimax Debug Funktion: Printed den Minimax Game Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_minimax(evaluate_minimax: Callable):\n",
    "    \"\"\"Prints the Minimax Game Tree.\"\"\"\n",
    "\n",
    "    def minimax_debug(\n",
    "        self,\n",
    "        minimax: Callable,\n",
    "        board: chess.Board,\n",
    "        current_evaluation: int,\n",
    "        limit: int,\n",
    "        level: int = 0,\n",
    "        alpha: int = -Exercise08AI.LIMIT,\n",
    "        beta: int = Exercise08AI.LIMIT,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        color = \"white\" if board.turn else \"black\"\n",
    "        ws = (level + 1) * \"    \"\n",
    "        key = self.get_key(board)\n",
    "        if key in self.cache:\n",
    "            print(\n",
    "                f\"level: {level} {ws} mm> {color}; α: {alpha}; β: {beta}; limit: {limit}\"\n",
    "            )  # memoization\n",
    "        else:\n",
    "            print(\n",
    "                f\"level: {level} {ws} --> {color}; α: {alpha}; β: {beta}; limit: {limit}\"\n",
    "            )\n",
    "        evaluation, best_move = evaluate_minimax(\n",
    "            self, minimax, board, current_evaluation, limit, level, alpha, beta\n",
    "        )\n",
    "        print(\n",
    "            f\"level: {level} {ws} <-- {color}; ev: {evaluation}; best_move: {best_move}\"\n",
    "        )\n",
    "        return evaluation, best_move\n",
    "\n",
    "    return minimax_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prüft ob der Zug ein Schlagzug, eine Umwandlung oder ein Schachzug ist und gibt True/False zurück. Push den Move dabei auf das Board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def is_quiet_move(self, board: chess.Board, move: chess.Move) -> bool:\n",
    "        \"\"\"Checks if the next move was an promotion, capture or check move. Pushes the given move on the board.\"\"\"\n",
    "        if move.promotion or board.piece_type_at(move.to_square):\n",
    "            board.push(move)\n",
    "            return False\n",
    "        board.push(move)\n",
    "        if board.is_check():\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neu:\n",
    "- Die Tiefe wird nun anders interpretiert und hochgezählt statt herunter. Das ist intuitiver und vereinfacht den Programmcode auch deshalb, weil dann nicht mit negativen Zahlen gerechnet werden muss.  Aus diesem Grund ist die Abbruchbedingung hier dann auch `level == limit` und nicht mehr `level == 0`.\n",
    "- Es gibt eine neue Abbruchbedingung wenn die maximale Tiefe erreicht ist. In diesem Fall soll die Kette von z.B. Schlagzügen nicht weiter untersucht werden. Da dieser Pfad dann unbekannt ist, soll er die jeweils schlechteste Wertung bekommen, damit er in nahezu keinem Fall gewählt wird. Diskussion: Alternativ könnte man auch das Board evaluieren und die Wertung zurückgeben, damit steigt aber wieder das Risiko eine Horizont-Effekts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def minimax_early_abort(\n",
    "        self, board: chess.Board, level: int, limit: int, current_evaluation: int\n",
    "    ) -> int | None:\n",
    "        \"\"\"Returns an evaluation iff the minimax has an early exit condition. Returns None otherwise.\"\"\"\n",
    "        is_checkmate = board.is_checkmate()\n",
    "        if is_checkmate and not board.turn:\n",
    "            # White has won the game\n",
    "            evaluation = self.LIMIT - level\n",
    "            return evaluation\n",
    "\n",
    "        elif is_checkmate and board.turn:\n",
    "            # Black has won the game\n",
    "            evaluation = -self.LIMIT + level\n",
    "            return evaluation\n",
    "\n",
    "        elif (\n",
    "            board.is_insufficient_material()\n",
    "            or not board.legal_moves\n",
    "            or board.is_fifty_moves()\n",
    "        ):\n",
    "            # Game is a draw\n",
    "            return 0\n",
    "\n",
    "        # Recursion abort case\n",
    "        if level == limit:\n",
    "            return current_evaluation\n",
    "\n",
    "        # Maximum level reached\n",
    "        if level == self.MAX_SVE_DEPTH:\n",
    "            # Avoid this potentially bad path\n",
    "            if board.turn:\n",
    "                return self.LIMIT - level\n",
    "            else:\n",
    "                return -self.LIMIT + level\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neu:\n",
    "- Die Funtkion Minimax wurde in die Funktionen maxValue, minValue und evaluate_minimax analog zum Skript aufgeteilt (im Skript heißt die nur `evaluate`, die ist bei uns aber schon belegt).\n",
    "- minValue und maxValue anthalten jeweils den ihren Zweig aus der ehemaligen minimax-Funktion\n",
    "- evaluate_minimax übernimmt nun die Memoisierung (kein Decorator mehr)\n",
    "- Neuer Parameter `limit` (Suchtiefe von `get_next_middle_game_move`), der Parameter `real_depth` wird nicht mehr benötigt, diesen Wert hat nun der Parameter `depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "\n",
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def maxValue(\n",
    "        self,\n",
    "        board: chess.Board,\n",
    "        current_evaluation: int,\n",
    "        limit: int,\n",
    "        level: int,\n",
    "        alpha: int,\n",
    "        beta: int,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        \"\"\"Searches the best value with given limit using minimax algorithm\"\"\"\n",
    "        early_abort_evaluation = self.minimax_early_abort(\n",
    "            board, level, limit, current_evaluation\n",
    "        )\n",
    "        if early_abort_evaluation is not None:\n",
    "            self.stats[-1][\"leaf_ctr\"] += 1\n",
    "            self.stats[-1][\"depth_sum\"] += level\n",
    "            return early_abort_evaluation, None\n",
    "\n",
    "        best_move = None\n",
    "        moves: list[tuple[int, int, chess.Move]] = []\n",
    "\n",
    "        # White to play (positive numbers are good)\n",
    "        for i, move in enumerate(board.legal_moves):\n",
    "            key = self.get_key(board, limit - level - 1)\n",
    "            old_eval = self.cache.get(key, (None, i))[1]\n",
    "            heapq.heappush(moves, (-old_eval, i, move))\n",
    "        maxEvaluation = alpha\n",
    "        while moves:\n",
    "            move = heapq.heappop(moves)[2]\n",
    "            if level + 1 < limit:\n",
    "                new_limit = limit\n",
    "                board.push(move)\n",
    "            elif self.is_quiet_move(board, move):\n",
    "                new_limit = limit\n",
    "            else:\n",
    "                new_limit = limit + 1\n",
    "            evaluation, _, = self.evaluate_minimax(\n",
    "                self.minValue,\n",
    "                board,\n",
    "                self.evaluate(board, current_evaluation),\n",
    "                new_limit,\n",
    "                level + 1,\n",
    "                maxEvaluation,\n",
    "                beta,\n",
    "            )\n",
    "            board.pop()\n",
    "            if evaluation >= beta:\n",
    "                return evaluation, move\n",
    "            if evaluation > maxEvaluation:\n",
    "                best_move = move\n",
    "                maxEvaluation = evaluation\n",
    "        return maxEvaluation, best_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def minValue(\n",
    "        self,\n",
    "        board: chess.Board,\n",
    "        current_evaluation: int,\n",
    "        limit: int,\n",
    "        level: int,\n",
    "        alpha: int,\n",
    "        beta: int,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        \"\"\"Searches the best value with given limit using minimax algorithm\"\"\"\n",
    "        early_abort_evaluation = self.minimax_early_abort(\n",
    "            board, level, limit, current_evaluation\n",
    "        )\n",
    "        if early_abort_evaluation is not None:\n",
    "            self.stats[-1][\"leaf_ctr\"] += 1\n",
    "            self.stats[-1][\"depth_sum\"] += level\n",
    "            return early_abort_evaluation, None\n",
    "\n",
    "        best_move = None\n",
    "        moves: list[tuple[int, int, chess.Move]] = []\n",
    "\n",
    "        # Black to play (negative numbers are good)\n",
    "        for i, move in enumerate(board.legal_moves):\n",
    "            key = self.get_key(board, limit - level - 1)\n",
    "            old_eval = self.cache.get(key, (None, i))[1]\n",
    "            heapq.heappush(moves, (old_eval, i, move))\n",
    "        minEvaluation = beta\n",
    "        while moves:\n",
    "            move = heapq.heappop(moves)[2]\n",
    "            if level + 1 < limit:\n",
    "                new_limit = limit\n",
    "                board.push(move)\n",
    "            elif self.is_quiet_move(board, move):\n",
    "                new_limit = limit\n",
    "            else:\n",
    "                new_limit = limit + 1\n",
    "            evaluation, _ = self.evaluate_minimax(\n",
    "                self.maxValue,\n",
    "                board,\n",
    "                self.evaluate(board, current_evaluation),\n",
    "                new_limit,\n",
    "                level + 1,\n",
    "                alpha,\n",
    "                minEvaluation,\n",
    "            )\n",
    "            board.pop()\n",
    "            if evaluation <= alpha:\n",
    "                return evaluation, move\n",
    "            if evaluation < minEvaluation:\n",
    "                best_move = move\n",
    "                minEvaluation = evaluation\n",
    "        return minEvaluation, best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neu:\n",
    "- `get_key`: Der cache key enthält keine depth mehr\n",
    "- `store_in_cache`: Die depth wird nun als Wert im Cache abgelegt (`limit - depth`)\n",
    "  - Beispiel: Eine Funktionsaufruf mit depth 5 und limit 7 führt dazu, dass von dieser Stellung aus alle Positionen bis zum limit untersucht werden. Von dieser Stellung aus gesehen, ergibt das dann eine Tiefe von 2, welche dann auch im Cache gespeichert wird (7-5).\n",
    "- `get_from_cache`: Hier wird neu überprüft ob die Tiefer größer ist als die Tiefe des Ergebnisses im Cache. Falls ja kann der Cache hier nicht verwendet werden und das Resultat muss erneut mit der höheren Tiefe berechnet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def get_key(self, board: chess.Board, depth: int) -> tuple:\n",
    "        \"\"\"Calculates a key that uniquely identifies a given board\"\"\"\n",
    "        return (\n",
    "            board.pawns,\n",
    "            board.knights,\n",
    "            board.bishops,\n",
    "            board.rooks,\n",
    "            board.queens,\n",
    "            board.kings,\n",
    "            board.occupied_co[chess.WHITE],\n",
    "            board.occupied_co[chess.BLACK],\n",
    "            board.turn,\n",
    "            board.castling_rights,\n",
    "            board.halfmove_clock if board.halfmove_clock > 42 else -42,\n",
    "            depth,\n",
    "        )\n",
    "\n",
    "    def store_in_cache(self, key: tuple, result: tuple, alpha: int, beta: int) -> None:\n",
    "        \"\"\"Stores the result of a minimax computation in the cache.\"\"\"\n",
    "        evaluation, move = result\n",
    "        if evaluation <= alpha:\n",
    "            self.cache[key] = (\"≤\", evaluation, move)\n",
    "        elif evaluation < beta:\n",
    "            self.cache[key] = (\"=\", evaluation, move)\n",
    "        else:\n",
    "            self.cache[key] = (\"≥\", evaluation, move)\n",
    "\n",
    "    def get_from_cache(\n",
    "        self,\n",
    "        minimax: Callable,\n",
    "        key: tuple,\n",
    "        board: chess.Board,\n",
    "        current_eval: int,\n",
    "        limit: int,\n",
    "        level: int,\n",
    "        alpha: int,\n",
    "        beta: int,\n",
    "    ) -> tuple:\n",
    "        \"\"\"Gets a result from the cache if possible.\"\"\"\n",
    "        flag, evaluation, move = self.cache[key]\n",
    "        if flag == \"=\":\n",
    "            return evaluation, move\n",
    "        elif flag == \"≤\":\n",
    "            if evaluation <= alpha:\n",
    "                return evaluation, move\n",
    "            elif evaluation < beta:\n",
    "                result = minimax(board, current_eval, limit, level, alpha, evaluation)\n",
    "                self.store_in_cache(key, result, alpha, evaluation)\n",
    "                return result\n",
    "            else:\n",
    "                result = minimax(board, current_eval, limit, level, alpha, beta)\n",
    "                self.store_in_cache(key, result, alpha, beta)\n",
    "                return result\n",
    "        else:\n",
    "            if evaluation <= alpha:\n",
    "                result = minimax(board, current_eval, limit, level, alpha, beta)\n",
    "                self.store_in_cache(key, result, alpha, beta)\n",
    "                return result\n",
    "            elif evaluation < beta:\n",
    "                result = minimax(board, current_eval, limit, level, evaluation, beta)\n",
    "                self.store_in_cache(key, result, evaluation, beta)\n",
    "                return result\n",
    "            else:\n",
    "                return evaluation, move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prinzipiell die gleiche Funktion wie vorher `memoize_minimax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    # @debug_minimax\n",
    "    def evaluate_minimax(\n",
    "        self,\n",
    "        minimax: Callable,\n",
    "        board: chess.Board,\n",
    "        current_evaluation: int,\n",
    "        limit: int,\n",
    "        level: int = 0,\n",
    "        alpha: int = -Exercise08AI.LIMIT,\n",
    "        beta: int = Exercise08AI.LIMIT,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        \"\"\"Searches the best value with given limit using minimax algorithm\"\"\"\n",
    "        key = self.get_key(board, limit - level)\n",
    "        self.stats[-1][\"cache_tries\"] += 1\n",
    "        self.stats[-1][\"max_depth\"] = max(level, self.stats[-1][\"max_depth\"])\n",
    "\n",
    "        if key in self.cache:\n",
    "            self.stats[-1][\"cache_hits\"] += 1\n",
    "            return self.get_from_cache(\n",
    "                minimax,\n",
    "                key,\n",
    "                board,\n",
    "                current_evaluation,\n",
    "                limit,\n",
    "                level,\n",
    "                alpha,\n",
    "                beta,\n",
    "            )\n",
    "        result = minimax(board, current_evaluation, limit, level, alpha, beta)\n",
    "        self.store_in_cache(key, result, alpha, beta)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neu:\n",
    "- Vor jeder Evaluierung wird geschaut ob der Spieler Weiß oder Schwarz ist. Je nachdem wird entweder die Referenz auf minValue oder maxValue übergeben. Ja, das ist eher hässlich und nicht so effizient, gerne noch anpassen. Prinzipiell steht ja schon nach dem ersten Zug die Farbe für das gesamt Spiel fest. Auch muss man das nicht innerhalb der Schleife prüfen. Wie gesagt, noch optimierungsfähig :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def get_next_middle_game_move(self, board: chess.Board) -> chess.Move:\n",
    "        \"\"\"Gets the best next move\"\"\"\n",
    "        self.last_evaluation: int | None  # type annotation for mypy\n",
    "        self.stats[-1][\"cache_hits\"] = 0\n",
    "        self.stats[-1][\"cache_tries\"] = 0\n",
    "        self.stats[-1][\"leaf_ctr\"] = 0\n",
    "        self.stats[-1][\"max_depth\"] = -1\n",
    "        self.stats[-1][\"avg_depth\"] = -1\n",
    "        self.stats[-1][\"depth_sum\"] = 0\n",
    "\n",
    "        if self.is_king_endgame != self.check_king_endgame(board):\n",
    "            self.last_evaluation += self.get_endgame_evaluation_change(board)\n",
    "        # Calculate current evaluation\n",
    "        if self.last_evaluation is None:  # type: ignore\n",
    "            current_evaluation = self.full_evaluate(board)\n",
    "        else:\n",
    "            # Get current evaluation (after opponent move)\n",
    "            current_evaluation = self.evaluate(board, self.last_evaluation)\n",
    "\n",
    "        for limit in range(1, self.DEPTH + 1):\n",
    "            # Call minimax and get best move\n",
    "            if board.turn:\n",
    "                future_evaluation, best_move = self.evaluate_minimax(\n",
    "                    self.maxValue, board, current_evaluation, limit\n",
    "                )\n",
    "            else:\n",
    "                future_evaluation, best_move = self.evaluate_minimax(\n",
    "                    self.minValue, board, current_evaluation, limit\n",
    "                )\n",
    "\n",
    "        # Debugging fail safe\n",
    "        assert best_move, f\"\"\"\n",
    "        Best move is None with fen '{board.fen()}' at player {type(self).__name__}! \n",
    "        depth: {self.DEPTH}, last_eval: {self.last_evaluation}, current_evaluation: {current_evaluation},\n",
    "        is_king_engame: {getattr(self, 'is_king_endgame', \"N/A\")}, move_stack: {board.move_stack}\n",
    "        \"\"\"\n",
    "        # Update last evaluation (after player move)\n",
    "        self.last_evaluation = current_evaluation + self.incremental_evaluate(\n",
    "            board, best_move\n",
    "        )\n",
    "        # Update stats\n",
    "        self.stats[-1][\"minimax_eval\"] = future_evaluation\n",
    "        self.stats[-1][\"board_eval_before_move\"] = current_evaluation\n",
    "        self.stats[-1][\"board_eval_after_move\"] = self.last_evaluation\n",
    "        self.stats[-1][\"avg_depth\"] = self.stats[-1][\"depth_sum\"] / (\n",
    "            self.stats[-1][\"leaf_ctr\"] or 1\n",
    "        )\n",
    "        del self.stats[-1][\"depth_sum\"]\n",
    "        del self.stats[-1][\"leaf_ctr\"]\n",
    "        self.stats[-1][\"cache_size_mb\"] = round(\n",
    "            sys.getsizeof(self.cache) / (1024 * 1024), 2\n",
    "        )\n",
    "        if board.is_irreversible(best_move):\n",
    "            self.cache.clear()\n",
    "            self.stats[-1][\"cache_cleared\"] = True\n",
    "            self.cache: dict[tuple, Any] = {}\n",
    "        else:\n",
    "            self.stats[-1][\"cache_cleared\"] = False\n",
    "        return best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Bereich\n",
    "\n",
    "Die folgenden Zellen enthalten Unit-Tests der oben implementierten Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Exercise03AI as Exercise03AITests\n",
    "import Exercise04AI as Exercise04AITests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create player and board\n",
    "unit_test_player = Exercise08AI(player_name=\"Ex08AI\", search_depth=2, max_depth=3)\n",
    "board = chess.Board(\"5rk1/1b3p2/8/3p4/3p2P1/2Q4B/5P1K/R3R3 b - - 0 36\")\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test minimax\n",
    "def test_minimax(unit_test_player: ChessAI, board: chess.Board):\n",
    "    unit_test_player.cache = {}  # Clear cache\n",
    "    unit_test_player.stats[-1][\"cache_tries\"] = 0\n",
    "    unit_test_player.stats[-1][\"cache_hits\"] = 0\n",
    "    unit_test_player.stats[-1][\"leaf_ctr\"] = 0\n",
    "    unit_test_player.stats[-1][\"max_depth\"] = 0\n",
    "    unit_test_player.stats[-1][\"depth_sum\"] = 0\n",
    "    f = unit_test_player.maxValue if board.turn else unit_test_player.minValue\n",
    "    mm_evaluation, mm_move = unit_test_player.evaluate_minimax(\n",
    "        f, board, current_evaluation=1240, limit=unit_test_player.DEPTH\n",
    "    )\n",
    "    print(f\"Minimax Evaluation: {mm_evaluation}\")\n",
    "    print(f\"Minimax Move: {mm_move}\")\n",
    "    assert mm_evaluation == 355, \"Minimax evaluation does not match expected value!\"\n",
    "    assert mm_move.uci() == \"d4c3\", \"Minimax move does not match expected value!\"\n",
    "    assert unit_test_player.cache != {}, \"Cache is empty!\"\n",
    "    print(f\"Elements in cache: {len(unit_test_player.cache)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_minimax(unit_test_player, board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test next move function (with memoized minimax result)\n",
    "Exercise03AITests.test_next_move(unit_test_player, board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporärer Bereich\n",
    "\n",
    "Der folgende Bereich dient zum temporären Debuggen und kann nicht-funktionierenden Code enthalten. Dieser Bereich wird vor der Abgabe entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
