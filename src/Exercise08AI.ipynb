{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".container {\n",
    "  width: 100%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from AIBaseClass import ChessAI\n",
    "from Exercise03AI import Exercise03AI\n",
    "from Exercise04AI import Exercise04AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 08: Minimax mit Alpha-Beta-Pruning, Memoisierung, Progressive Deepening und Singular Value Extension\n",
    "\n",
    "Dieses Notebook erweitert den Minimax-Algorithmus um die Singular Value Extension. Hierbei wird die Suchtiefe im jeweiligen Teilbaum um 1 erhöht, wenn entweder eine Figur geschlagen wurde, der König im Schach steht oder eine Figurenumwandlung durchgeführt wurde. Die Singular Value Extension reduziert den sogenannten Horizont-Effekt, d.h. eine gute Einschätzung eines Boards am Tiefenlimit, wobei es aber einen nächsten, nicht-untersuchten Zug gibt, mit welchem sich die Stellung stark verschlechtert. Die Erhöhung wird dabei nur bis zu einem definierten Limit (`MAX_SVE_DEPTH`) durchgeführt, da ansonsten zu viele Ressourcen benötigt werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "from typing import Any, Callable\n",
    "\n",
    "\n",
    "class Exercise08AI(Exercise03AI):\n",
    "    \"\"\"Chooses middle game moves using minimax algorithm, alpha-beta-pruning,\n",
    "    memoization and progressive deepening and the quiescence search\"\"\"\n",
    "\n",
    "    def __init__(self, max_depth: int = 10, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.cache: dict[tuple, Any] = {}\n",
    "        assert (\n",
    "            self.DEPTH < max_depth\n",
    "        ), \"Max depth needs to be greater than search depth!\"\n",
    "        self.MAX_SVE_DEPTH: int = max_depth\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resets all internal variables\"\"\"\n",
    "        super().reset()\n",
    "        self.cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `debug_minimax` ist eine Debugging-Hilfsfunktion um die Berechnungen der `evaluate_minimax`-Funktion nachvollziehen zu können. Die Funktion wird bei Bedarf als Dekorator auf die Funktion `evaluate_minimax` angewendet, ist aber standardmäßig nicht aktiviert. Die Parameter sind dieselben wie bei der Funktion `evaluate_minimax` und werden im dortigen Abschnitt beschrieben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_minimax(evaluate_minimax: Callable):\n",
    "    \"\"\"Prints the Minimax Game Tree.\"\"\"\n",
    "\n",
    "    def minimax_debug(\n",
    "        self,\n",
    "        minimax: Callable,\n",
    "        board: chess.Board,\n",
    "        current_evaluation: int,\n",
    "        limit: int,\n",
    "        level: int = 0,\n",
    "        alpha: int = -Exercise08AI.LIMIT,\n",
    "        beta: int = Exercise08AI.LIMIT,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        color = \"white\" if board.turn else \"black\"\n",
    "        ws = (level + 1) * \"    \"\n",
    "        key = Exercise04AI.get_key(board)\n",
    "        if key in self.cache:\n",
    "            print(\n",
    "                f\"level: {level} {ws} mm> {color}; α: {alpha}; β: {beta}; limit: {limit}\"\n",
    "            )  # memoization\n",
    "        else:\n",
    "            print(\n",
    "                f\"level: {level} {ws} --> {color}; α: {alpha}; β: {beta}; limit: {limit}\"\n",
    "            )\n",
    "        evaluation, best_move = evaluate_minimax(\n",
    "            self, minimax, board, current_evaluation, limit, level, alpha, beta\n",
    "        )\n",
    "        print(\n",
    "            f\"level: {level} {ws} <-- {color}; ev: {evaluation}; best_move: {best_move}\"\n",
    "        )\n",
    "        return evaluation, best_move\n",
    "\n",
    "    return minimax_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `is_quiet_move_and_push` ist ein Indikator dafür, ob die Suchtiefe (`limit`) erhöht werden muss. Dabei prüft die Funktion ob der übergebene Zug ein Schlagzug, eine Umwandlung oder ein Schachzug ist und gibt entsprechend `True` bzw. `False` zurück. Der übergebene Zug wird im Laufe der Prüfung auf das Board angewendet.\n",
    "\n",
    "Hinweis: Die `chess`-Bibliothek bietet auch die Funktion `gives_check` an, welche überprüft ob ein übergebener Zug den Gegner in Schach setzt. Da diese Funktion aber intern nur den Zug anwendet, `is_check` aufruft und den Zug anschließend wieder entfernt wurde aus Effizienzgründen direkt letztere Funktion verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def is_quiet_move_and_push(self, board: chess.Board, move: chess.Move) -> bool:\n",
    "        \"\"\"Checks if the next move was an promotion, capture or check move. Pushes the given move on the board.\"\"\"\n",
    "        if move.promotion or board.piece_type_at(move.to_square):\n",
    "            board.push(move)\n",
    "            return False\n",
    "        board.push(move)\n",
    "        if board.is_check():\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Singular Value Extension erfordert es, dass das Limit für die Suche nun erstmals laufend angepasst werden muss. Um negative Zahlen zu vermeiden, eine statistische Erfassung zu ermöglichen und aus Gründen der Verständlichkeit wird daher der Parameter `depth`, welcher bisher zum Zielwert `0` dekrementiert wurde nun durch die Parameter `level` und `limit` ersetzt. Der Parameter `limit` gibt dabei die gewünschte Suchtiefe an und kann im Laufe der Berechnung durch die SVE erhöht werden. Das Argument `level` ist nun die aktuelle Suchtiefe, welche bei `0` startet und mit jedem Aufruf von `minValue` oder `maxValue` inkrementiert wird.\n",
    "\n",
    "Für die SVE ist eine neue Abbruchbedingung bei Erreichen der maximalen Tiefe notwendig. Diese wird hier in der Funktion `minimax_early_abort` hinzugefügt. Parallel dazu ermöglichst die Veränderung der Parameter (vorheriger Absatz) einige Vereinfachungen, welche in dieser überschriebenen Variante implementiert sind.\n",
    "\n",
    "Die folgenden Änderungen wurden vorgenommen:\n",
    "- Die Abbruchbedingung wurde auf die neuen Parameter angepasst und lautet nun `level == limit` statt wie bisher `depth == 0`.\n",
    "- Es gibt eine zusätzliche Abbruchbedingung sobald die maximale Tiefe für die Singular-Value-Extension erreicht ist. In diesem Fall wird die Evaluierung des aktuellen Pfades beendet, auch wenn dieser sich noch in einer Kette von nicht-ruhigen Zügen befindet. Da der Pfad nicht vollständig evaluiert wurde und somit ungewiss ist, wird er mit dem jeweils schlechtesten Wert versehen. Somit werden unvollständig untersuchte Pfade immer vermieden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def minimax_early_abort(\n",
    "        self, board: chess.Board, level: int, limit: int, current_evaluation: int\n",
    "    ) -> int | None:\n",
    "        \"\"\"Returns an evaluation iff the minimax has an early exit condition. Returns None otherwise.\"\"\"\n",
    "        is_checkmate = board.is_checkmate()\n",
    "        if is_checkmate and not board.turn:\n",
    "            # White has won the game\n",
    "            evaluation = self.LIMIT - level\n",
    "            return evaluation\n",
    "\n",
    "        elif is_checkmate and board.turn:\n",
    "            # Black has won the game\n",
    "            evaluation = -self.LIMIT + level\n",
    "            return evaluation\n",
    "\n",
    "        elif (\n",
    "            board.is_insufficient_material()\n",
    "            or not board.legal_moves\n",
    "            or board.is_fifty_moves()\n",
    "        ):\n",
    "            # Game is a draw\n",
    "            return 0\n",
    "\n",
    "        # Recursion abort case\n",
    "        if level == limit:\n",
    "            return current_evaluation\n",
    "\n",
    "        # Maximum level reached\n",
    "        if level == self.MAX_SVE_DEPTH:\n",
    "            # Avoid this potentially bad path\n",
    "            if board.turn:\n",
    "                return self.LIMIT - level\n",
    "            else:\n",
    "                return -self.LIMIT + level\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Lesbarkeit der Minimax Funktion zu wahren, wurde diese in die Funktionen `maxValue`, `minValue` und `evaluate_minimax` aufgeteilt, wobei `maxValue` und `minValue` die beiden Zweige des Minimax Algorithmus darstellen.\n",
    "Die `evaluate_minimax` übernimmt nun die Memoisierung, wodurch kein Dekorator mehr benötigt wird.\n",
    "\n",
    "Die Funtion `maxValue` berechnet wie bisher die maximale Evaluierung aller möglichen Züge. Neu hinzugekommen ist die laufende Erhöhung des Limits, welche direkt nach dem Entfernen eines Zuges von der Prioritätswarteschlange `moves` durchgeführt wird. Dort werden nun drei Fälle unterschieden:\n",
    "- Wenn die Ungleichung `level + 1 < limit` erfüllt ist, wird das Limit grundsätzlich nicht erhöht. Dies hat zur Folge das die SVE erst zum Einsatz kommt, wenn die Zielsuchtiefe im nächsten Schritt erreicht ist und somit die Suche sonst zu Ende wäre. Diese Bedingung wurde aus Performance-Gründen mit aufgenommen, da die Berechnungen ansonsten durch die teilweise sehr hohe Tiefe wesentlich länger dauern.\n",
    "- Wenn der Zug \"ruhig\" ist, d.h. es sich nicht um einen Schlagzug, eine Umwandlung oder ein Schachzug handelt, wird das Limit nicht erhöht.\n",
    "- Falls die ersten beiden Bedingungen nicht zutreffen, wird die SVE durchgeführt und das Limit um `1` erhöht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "\n",
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def maxValue(\n",
    "        self,\n",
    "        board: chess.Board,\n",
    "        current_evaluation: int,\n",
    "        limit: int,\n",
    "        level: int,\n",
    "        alpha: int,\n",
    "        beta: int,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        \"\"\"Searches the best value with given limit using minimax algorithm\"\"\"\n",
    "        early_abort_evaluation = self.minimax_early_abort(\n",
    "            board, level, limit, current_evaluation\n",
    "        )\n",
    "        if early_abort_evaluation is not None:\n",
    "            self.stats[-1][\"leaf_ctr\"] += 1\n",
    "            self.stats[-1][\"depth_sum\"] += level  # for calculating the average depth\n",
    "            return early_abort_evaluation, None\n",
    "\n",
    "        best_move = None\n",
    "        moves: list[tuple[int, int, chess.Move]] = []\n",
    "\n",
    "        # White to play (positive numbers are good)\n",
    "        for i, move in enumerate(board.legal_moves):\n",
    "            key = Exercise04AI.get_key(board, limit - level - 1)\n",
    "            old_eval = self.cache.get(key, (None, i))[1]\n",
    "            heapq.heappush(moves, (-old_eval, i, move))\n",
    "        maxEvaluation = alpha\n",
    "        while moves:\n",
    "            move = heapq.heappop(moves)[2]\n",
    "            if level + 1 < limit:\n",
    "                new_limit = limit\n",
    "                board.push(move)\n",
    "            elif self.is_quiet_move_and_push(board, move):\n",
    "                new_limit = limit\n",
    "            else:\n",
    "                new_limit = limit + 1\n",
    "            evaluation, _, = self.evaluate_minimax(\n",
    "                self.minValue,\n",
    "                board,\n",
    "                self.evaluate(board, current_evaluation),\n",
    "                new_limit,\n",
    "                level + 1,\n",
    "                maxEvaluation,\n",
    "                beta,\n",
    "            )\n",
    "            board.pop()\n",
    "            if evaluation >= beta:\n",
    "                return evaluation, move\n",
    "            if evaluation > maxEvaluation:\n",
    "                best_move = move\n",
    "                maxEvaluation = evaluation\n",
    "        return maxEvaluation, best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funtion `minValue` berechnet wie bisher die minimale Evaluierung aller möglichen Züge und enthält ansonsten dieselben Änderungen wie die Funktion `maxValue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def minValue(\n",
    "        self,\n",
    "        board: chess.Board,\n",
    "        current_evaluation: int,\n",
    "        limit: int,\n",
    "        level: int,\n",
    "        alpha: int,\n",
    "        beta: int,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        \"\"\"Searches the best value with given limit using minimax algorithm\"\"\"\n",
    "        early_abort_evaluation = self.minimax_early_abort(\n",
    "            board, level, limit, current_evaluation\n",
    "        )\n",
    "        if early_abort_evaluation is not None:\n",
    "            self.stats[-1][\"leaf_ctr\"] += 1\n",
    "            self.stats[-1][\"depth_sum\"] += level\n",
    "            return early_abort_evaluation, None\n",
    "\n",
    "        best_move = None\n",
    "        moves: list[tuple[int, int, chess.Move]] = []\n",
    "\n",
    "        # Black to play (negative numbers are good)\n",
    "        for i, move in enumerate(board.legal_moves):\n",
    "            key = Exercise04AI.get_key(board, limit - level - 1)\n",
    "            old_eval = self.cache.get(key, (None, i))[1]\n",
    "            heapq.heappush(moves, (old_eval, i, move))\n",
    "        minEvaluation = beta\n",
    "        while moves:\n",
    "            move = heapq.heappop(moves)[2]\n",
    "            if level + 1 < limit:\n",
    "                new_limit = limit\n",
    "                board.push(move)\n",
    "            elif self.is_quiet_move_and_push(board, move):\n",
    "                new_limit = limit\n",
    "            else:\n",
    "                new_limit = limit + 1\n",
    "            evaluation, _ = self.evaluate_minimax(\n",
    "                self.maxValue,\n",
    "                board,\n",
    "                self.evaluate(board, current_evaluation),\n",
    "                new_limit,\n",
    "                level + 1,\n",
    "                alpha,\n",
    "                minEvaluation,\n",
    "            )\n",
    "            board.pop()\n",
    "            if evaluation <= alpha:\n",
    "                return evaluation, move\n",
    "            if evaluation < minEvaluation:\n",
    "                best_move = move\n",
    "                minEvaluation = evaluation\n",
    "        return minEvaluation, best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Methode `store_in_cache` ist identisch zu der Implementierung in `Exercise06AI`. In der Methode `get_from_cache` existiert der minimale Unterschied, dass anstatt des `depth`-Parameters die Argumente `limit` und `level` durchgereicht werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def store_in_cache(self, key: tuple, result: tuple, alpha: int, beta: int) -> None:\n",
    "        \"\"\"Stores the result of a minimax computation in the cache.\"\"\"\n",
    "        evaluation, move = result\n",
    "        if evaluation <= alpha:\n",
    "            self.cache[key] = (\"≤\", evaluation, move)\n",
    "        elif evaluation < beta:\n",
    "            self.cache[key] = (\"=\", evaluation, move)\n",
    "        else:\n",
    "            self.cache[key] = (\"≥\", evaluation, move)\n",
    "\n",
    "    def get_from_cache(\n",
    "        self,\n",
    "        minimax: Callable,\n",
    "        key: tuple,\n",
    "        board: chess.Board,\n",
    "        current_eval: int,\n",
    "        limit: int,\n",
    "        level: int,\n",
    "        alpha: int,\n",
    "        beta: int,\n",
    "    ) -> tuple:\n",
    "        \"\"\"Gets a result from the cache if possible.\"\"\"\n",
    "        flag, evaluation, move = self.cache[key]\n",
    "        if flag == \"=\":\n",
    "            return evaluation, move\n",
    "        elif flag == \"≤\":\n",
    "            if evaluation <= alpha:\n",
    "                return evaluation, move\n",
    "            elif evaluation < beta:\n",
    "                result = minimax(board, current_eval, limit, level, alpha, evaluation)\n",
    "                self.store_in_cache(key, result, alpha, evaluation)\n",
    "                return result\n",
    "            else:\n",
    "                result = minimax(board, current_eval, limit, level, alpha, beta)\n",
    "                self.store_in_cache(key, result, alpha, beta)\n",
    "                return result\n",
    "        else:\n",
    "            if evaluation <= alpha:\n",
    "                result = minimax(board, current_eval, limit, level, alpha, beta)\n",
    "                self.store_in_cache(key, result, alpha, beta)\n",
    "                return result\n",
    "            elif evaluation < beta:\n",
    "                result = minimax(board, current_eval, limit, level, evaluation, beta)\n",
    "                self.store_in_cache(key, result, evaluation, beta)\n",
    "                return result\n",
    "            else:\n",
    "                return evaluation, move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Methode `evaluate_minimax` ist größtenteil identisch zu der bisherigen `memoize_minimax` Funktion.  \n",
    "Folgende Änderungen wurden eingeführt:\n",
    "- Die Funktion wird nicht mehr als Dekorator implementiert sondern direkt aufgerufen.\n",
    "- Da die Funktion `minimax` in zwei separate Funktionen aufgeteilt wurde, gibt es nun einen neuen Parameter `minimax`, welcher einen Zeiger auf die Funktion `minValue` oder `maxValue` enthält.\n",
    "- Statt mit dem Parameter `depth` wird die `get_key` Funktion intern mit der Differenz aus `limit` und `level` aufgerufen. Die Differenz ist dabei das zum aktuellen Board relative Limit , also mit welcher Tiefe die Suche vom aktuellen Board ausgehend durchgeführt werden soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    # @debug_minimax\n",
    "    def evaluate_minimax(\n",
    "        self,\n",
    "        minimax: Callable,\n",
    "        board: chess.Board,\n",
    "        current_evaluation: int,\n",
    "        limit: int,\n",
    "        level: int = 0,\n",
    "        alpha: int = -Exercise08AI.LIMIT,\n",
    "        beta: int = Exercise08AI.LIMIT,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        \"\"\"Searches the best value with given limit using minimax algorithm\"\"\"\n",
    "        key = Exercise04AI.get_key(board, limit - level)\n",
    "        self.stats[-1][\"cache_tries\"] += 1\n",
    "        self.stats[-1][\"max_depth\"] = max(level, self.stats[-1][\"max_depth\"])\n",
    "\n",
    "        if key in self.cache:\n",
    "            self.stats[-1][\"cache_hits\"] += 1\n",
    "            return self.get_from_cache(\n",
    "                minimax,\n",
    "                key,\n",
    "                board,\n",
    "                current_evaluation,\n",
    "                limit,\n",
    "                level,\n",
    "                alpha,\n",
    "                beta,\n",
    "            )\n",
    "        result = minimax(board, current_evaluation, limit, level, alpha, beta)\n",
    "        self.store_in_cache(key, result, alpha, beta)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Methode `get_next_middle_game_move` wird nun bei jedem Zug überprüft, welcher Spieler an der Reihe ist. Abhängig davon wird `evaluate_minimax` entweder mit `minValue` oder mit `maxValue` aufgerufen, um den besten Zug zu ermitteln. Alle sonstigen Änderungen dienen nur der genaueren Erfassung von statistischen Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def get_next_middle_game_move(self, board: chess.Board) -> chess.Move:\n",
    "        \"\"\"Gets the best next move\"\"\"\n",
    "        self.last_evaluation: int | None  # type annotation for mypy\n",
    "        self.stats[-1][\"cache_hits\"] = 0\n",
    "        self.stats[-1][\"cache_tries\"] = 0\n",
    "        self.stats[-1][\"leaf_ctr\"] = 0\n",
    "        self.stats[-1][\"max_depth\"] = -1\n",
    "        self.stats[-1][\"avg_depth\"] = -1\n",
    "        self.stats[-1][\"depth_sum\"] = 0\n",
    "\n",
    "        if self.is_king_endgame != self.check_king_endgame(board):\n",
    "            self.last_evaluation += self.get_endgame_evaluation_change(board)\n",
    "        # Calculate current evaluation\n",
    "        if self.last_evaluation is None:  # type: ignore\n",
    "            current_evaluation = self.full_evaluate(board)\n",
    "        else:\n",
    "            # Get current evaluation (after opponent move)\n",
    "            current_evaluation = self.evaluate(board, self.last_evaluation)\n",
    "\n",
    "        evaluation_function = self.maxValue if board.turn else self.minValue\n",
    "        for limit in range(1, self.DEPTH + 1):\n",
    "            # Call minimax and get best move\n",
    "            future_evaluation, best_move = self.evaluate_minimax(\n",
    "                evaluation_function, board, current_evaluation, limit\n",
    "            )\n",
    "\n",
    "        # Debugging fail safe\n",
    "        assert best_move, f\"\"\"\n",
    "        Best move is None with fen '{board.fen()}' at player {type(self).__name__}! \n",
    "        depth: {self.DEPTH}, last_eval: {self.last_evaluation}, current_evaluation: {current_evaluation},\n",
    "        is_king_engame: {getattr(self, 'is_king_endgame', \"N/A\")}, move_stack: {board.move_stack}\n",
    "        \"\"\"\n",
    "        # Update last evaluation (after player move)\n",
    "        self.last_evaluation = current_evaluation + self.incremental_evaluate(\n",
    "            board, best_move\n",
    "        )\n",
    "        # Update stats\n",
    "        self.stats[-1][\"minimax_eval\"] = future_evaluation\n",
    "        self.stats[-1][\"board_eval_before_move\"] = current_evaluation\n",
    "        self.stats[-1][\"board_eval_after_move\"] = self.last_evaluation\n",
    "        self.stats[-1][\"avg_depth\"] = self.stats[-1][\"depth_sum\"] / (\n",
    "            self.stats[-1][\"leaf_ctr\"] or 1\n",
    "        )\n",
    "        del self.stats[-1][\"depth_sum\"]\n",
    "        del self.stats[-1][\"leaf_ctr\"]\n",
    "        self.stats[-1][\"cache_size_mb\"] = round(\n",
    "            sys.getsizeof(self.cache) / (1024 * 1024), 2\n",
    "        )\n",
    "        if board.is_irreversible(best_move):\n",
    "            self.cache.clear()\n",
    "            self.stats[-1][\"cache_cleared\"] = True\n",
    "        else:\n",
    "            self.stats[-1][\"cache_cleared\"] = False\n",
    "        return best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Bereich\n",
    "\n",
    "Die folgenden Zellen enthalten Unit-Tests der oben implementierten Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Exercise03AI as Exercise03AITests\n",
    "import Exercise04AI as Exercise04AITests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create player and board\n",
    "unit_test_player = Exercise08AI(player_name=\"Ex08AI\", search_depth=2, max_depth=3)\n",
    "board = chess.Board(\"5rk1/1b3p2/8/3p4/3p2P1/2Q4B/5P1K/R3R3 b - - 0 36\")\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test minimax\n",
    "def test_minimax(unit_test_player: ChessAI, board: chess.Board):\n",
    "    unit_test_player.cache = {}  # Clear cache\n",
    "    unit_test_player.stats[-1][\"cache_tries\"] = 0\n",
    "    unit_test_player.stats[-1][\"cache_hits\"] = 0\n",
    "    unit_test_player.stats[-1][\"leaf_ctr\"] = 0\n",
    "    unit_test_player.stats[-1][\"max_depth\"] = 0\n",
    "    unit_test_player.stats[-1][\"depth_sum\"] = 0\n",
    "    f = unit_test_player.maxValue if board.turn else unit_test_player.minValue\n",
    "    mm_evaluation, mm_move = unit_test_player.evaluate_minimax(\n",
    "        f, board, current_evaluation=1240, limit=unit_test_player.DEPTH\n",
    "    )\n",
    "    print(f\"Minimax Evaluation: {mm_evaluation}\")\n",
    "    print(f\"Minimax Move: {mm_move}\")\n",
    "    assert mm_evaluation == 355, \"Minimax evaluation does not match expected value!\"\n",
    "    assert mm_move.uci() == \"d4c3\", \"Minimax move does not match expected value!\"\n",
    "    assert unit_test_player.cache != {}, \"Cache is empty!\"\n",
    "    print(f\"Elements in cache: {len(unit_test_player.cache)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_minimax(unit_test_player, board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test next move function (with memoized minimax result)\n",
    "Exercise03AITests.test_next_move(unit_test_player, board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporärer Bereich\n",
    "\n",
    "Der folgende Bereich dient zum temporären Debuggen und kann nicht-funktionierenden Code enthalten. Dieser Bereich wird vor der Abgabe entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
