{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".container {\n",
    "  width: 100%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from AIBaseClass import ChessAI\n",
    "from Exercise04AI import Exercise04AI\n",
    "from Exercise06AI import Exercise06AI\n",
    "from Exercise07AI import Exercise07AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 08: Minimax mit Alpha-Beta-Pruning, Memoisierung, Progressive Deepening und Singular Value Extension\n",
    "\n",
    "Dieses Notebook erweitert den Minimax-Algorithmus um die Singular Value Extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class Exercise08AI(Exercise07AI):\n",
    "    \"\"\"Chooses middle game moves using minimax algorithm, alpha-beta-pruning,\n",
    "    memoization and progressive deepening and the quiescence search\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.real_depth: int = 0\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resets all internal variables\"\"\"\n",
    "        super().reset()\n",
    "        self.real_depth = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable\n",
    "\n",
    "\n",
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    @staticmethod\n",
    "    def memoize_minimax(minimax: Callable):\n",
    "        def minimax_memoized(\n",
    "            self,\n",
    "            board: chess.Board,\n",
    "            depth: int,\n",
    "            current_evaluation: int,\n",
    "            alpha: int = -Exercise08AI.LIMIT,\n",
    "            beta: int = Exercise08AI.LIMIT,\n",
    "            real_depth: int = -1,\n",
    "        ):\n",
    "            key = Exercise04AI.get_key(board, depth)\n",
    "            self.stats[-1][\"cache_tries\"] += 1\n",
    "\n",
    "            if key in self.cache:\n",
    "                self.stats[-1][\"cache_hits\"] += 1\n",
    "                return self.get_from_cache(\n",
    "                    minimax,\n",
    "                    key,\n",
    "                    board,\n",
    "                    depth,\n",
    "                    current_evaluation,\n",
    "                    alpha,\n",
    "                    beta,\n",
    "                    real_depth,\n",
    "                )\n",
    "            result = minimax(\n",
    "                self, board, depth, current_evaluation, alpha, beta, real_depth\n",
    "            )\n",
    "            self.stats[-1][\"max_depth\"] = max(result[2], self.stats[-1][\"max_depth\"])\n",
    "            self.store_in_cache(key, result, alpha, beta)\n",
    "            return result\n",
    "\n",
    "        return minimax_memoized\n",
    "\n",
    "    def store_in_cache(self, key: tuple, result: tuple, alpha: int, beta: int) -> None:\n",
    "        \"\"\"Stores the result of a minimax computation in the cache.\"\"\"\n",
    "        evaluation, move, eval_depth = result\n",
    "        if evaluation < alpha:\n",
    "            self.cache[key] = (\"≤\", evaluation, move, eval_depth)\n",
    "        elif evaluation <= beta:\n",
    "            self.cache[key] = (\"=\", evaluation, move, eval_depth)\n",
    "        else:\n",
    "            self.cache[key] = (\"≥\", evaluation, move, eval_depth)\n",
    "\n",
    "    def get_from_cache(\n",
    "        self,\n",
    "        minimax: Callable,\n",
    "        key: tuple,\n",
    "        board: chess.Board,\n",
    "        depth: int,\n",
    "        current_eval: int,\n",
    "        alpha: int,\n",
    "        beta: int,\n",
    "        real_depth: int,\n",
    "    ) -> tuple:\n",
    "        \"\"\"Gets a result from the cache if possible.\"\"\"\n",
    "        flag, evaluation, move, eval_depth = self.cache[key]\n",
    "        if flag == \"=\":\n",
    "            return evaluation, move, eval_depth\n",
    "        elif flag == \"≤\":\n",
    "            if evaluation <= alpha:\n",
    "                return evaluation, move, eval_depth\n",
    "            elif evaluation < beta:\n",
    "                result = minimax(\n",
    "                    self, board, depth, current_eval, alpha, evaluation, real_depth\n",
    "                )\n",
    "                self.store_in_cache(key, result, alpha, evaluation)\n",
    "                return result\n",
    "            else:\n",
    "                result = minimax(\n",
    "                    self, board, depth, current_eval, alpha, beta, real_depth\n",
    "                )\n",
    "                self.store_in_cache(key, result, alpha, beta)\n",
    "                return result\n",
    "        else:\n",
    "            if evaluation <= alpha:\n",
    "                result = minimax(\n",
    "                    self, board, depth, current_eval, alpha, beta, real_depth\n",
    "                )\n",
    "                self.store_in_cache(key, result, alpha, beta)\n",
    "                return result\n",
    "            elif evaluation < beta:\n",
    "                result = minimax(\n",
    "                    self, board, depth, current_eval, evaluation, beta, real_depth\n",
    "                )\n",
    "                self.store_in_cache(key, result, evaluation, beta)\n",
    "                return result\n",
    "            else:\n",
    "                return evaluation, move, eval_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_minimax(minimax: Callable):\n",
    "    \"\"\"Prints the Minimax Game Tree.\"\"\"\n",
    "\n",
    "    def minimax_debug(\n",
    "        self,\n",
    "        board: chess.Board,\n",
    "        depth: int,\n",
    "        current_evaluation: int,\n",
    "        alpha: int = -Exercise08AI.LIMIT,\n",
    "        beta: int = Exercise08AI.LIMIT,\n",
    "        real_depth: int = -1,\n",
    "    ):\n",
    "        color = \"white\" if board.turn else \"black\"\n",
    "        ws = (real_depth + 1) * \"    \"\n",
    "        key = Exercise04AI.get_key(board, depth)\n",
    "        if key in self.cache:\n",
    "            print(\n",
    "                f\"real_depth: {real_depth} {ws} mm> {color}; α: {alpha}; β: {beta}; depth: {depth}\"\n",
    "            )  # memoization\n",
    "        else:\n",
    "            print(\n",
    "                f\"real_depth: {real_depth} {ws} --> {color}; α: {alpha}; β: {beta}; depth: {depth}\"\n",
    "            )\n",
    "        evaluation, best_move, eval_depth = minimax(\n",
    "            self, board, depth, current_evaluation, alpha, beta, real_depth\n",
    "        )\n",
    "        print(\n",
    "            f\"real_depth: {real_depth} {ws} <-- {color}; ev: {evaluation}; ev_depth: {eval_depth}; {best_move}\"\n",
    "        )\n",
    "        return evaluation, best_move, eval_depth\n",
    "\n",
    "    return minimax_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def push_and_diff(self, board: chess.Board, move: chess.Move) -> int:\n",
    "        \"\"\"Pushes the given move on the board and calculates the depth change.\"\"\"\n",
    "        if move.promotion:\n",
    "            board.push(move)\n",
    "            return 0\n",
    "        if board.piece_type_at(move.to_square):\n",
    "            board.push(move)\n",
    "            return 0\n",
    "        board.push(move)\n",
    "        if board.is_check():\n",
    "            return 0\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import heapq\n",
    "\n",
    "\n",
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    #@debug_minimax\n",
    "    @Exercise08AI.memoize_minimax\n",
    "    def minimax(\n",
    "        self,\n",
    "        board: chess.Board,\n",
    "        depth: int,\n",
    "        current_evaluation: int,\n",
    "        alpha: int = -Exercise07AI.LIMIT,\n",
    "        beta: int = Exercise07AI.LIMIT,\n",
    "        real_depth: int = -1,\n",
    "    ) -> tuple[int, chess.Move | None, int]:\n",
    "        \"\"\"Searches the best value with given depth using minimax algorithm\"\"\"\n",
    "        real_depth += 1\n",
    "        early_abort_evaluation = self.minimax_early_abort(\n",
    "            board, depth, current_evaluation\n",
    "        )\n",
    "        if early_abort_evaluation is not None:\n",
    "            self.stats[-1][\"leaf_ctr\"] += 1\n",
    "            self.stats[-1][\"total_depth\"] += real_depth\n",
    "            return early_abort_evaluation, board.peek(), real_depth\n",
    "\n",
    "        best_move = None\n",
    "\n",
    "        # White to play (positive numbers are good)\n",
    "        moves: list[tuple[int, int, chess.Move]] = []\n",
    "        if board.turn:\n",
    "            for i, move in enumerate(board.legal_moves):\n",
    "                key = Exercise04AI.get_key(board, depth - 2)\n",
    "                old_eval = self.cache.get(key, (None, i))[1]\n",
    "                heapq.heappush(moves, (-old_eval, i, move))\n",
    "            maxEvaluation = -self.LIMIT\n",
    "            while moves:\n",
    "                move = heapq.heappop(moves)[2]\n",
    "                diff = self.push_and_diff(board, move)\n",
    "                if diff == 0 and real_depth >= self.DEPTH:\n",
    "                    new_depth = 0\n",
    "                else:\n",
    "                    new_depth = depth - diff\n",
    "                evaluation, _, eval_depth = self.minimax(\n",
    "                    board,\n",
    "                    new_depth,\n",
    "                    self.evaluate(board, current_evaluation),\n",
    "                    alpha,\n",
    "                    beta,\n",
    "                    real_depth,\n",
    "                )\n",
    "                board.pop()\n",
    "                if evaluation >= beta:\n",
    "                    return evaluation, move, eval_depth\n",
    "                if evaluation > maxEvaluation or (\n",
    "                    evaluation == maxEvaluation and best_move is None\n",
    "                ):\n",
    "                    best_move = move\n",
    "                    maxEvaluation = evaluation\n",
    "                alpha = max(alpha, evaluation)\n",
    "            return maxEvaluation, best_move, eval_depth\n",
    "\n",
    "        # Black to play (negative numbers are good)\n",
    "        else:\n",
    "            for i, move in enumerate(board.legal_moves):\n",
    "                key = Exercise04AI.get_key(board, depth - 2)\n",
    "                old_eval = self.cache.get(key, (None, i))[1]\n",
    "                heapq.heappush(moves, (old_eval, i, move))\n",
    "            minEvaluation = self.LIMIT\n",
    "            while moves:\n",
    "                move = heapq.heappop(moves)[2]\n",
    "                diff = self.push_and_diff(board, move)\n",
    "                if diff == 0 and real_depth >= self.DEPTH:\n",
    "                    new_depth = 0\n",
    "                else:\n",
    "                    new_depth = depth - diff\n",
    "                evaluation, _, eval_depth = self.minimax(\n",
    "                    board,\n",
    "                    new_depth,\n",
    "                    self.evaluate(board, current_evaluation),\n",
    "                    alpha,\n",
    "                    beta,\n",
    "                    real_depth,\n",
    "                )\n",
    "                board.pop()\n",
    "                if evaluation <= alpha:\n",
    "                    return evaluation, move, eval_depth\n",
    "                if evaluation < minEvaluation or (\n",
    "                    evaluation == minEvaluation and best_move is None\n",
    "                ):\n",
    "                    best_move = move\n",
    "                    minEvaluation = evaluation\n",
    "                beta = min(beta, evaluation)\n",
    "            return minEvaluation, best_move, eval_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "\n",
    "class Exercise08AI(Exercise08AI):  # type: ignore\n",
    "    def get_next_middle_game_move(self, board: chess.Board) -> chess.Move:\n",
    "        \"\"\"Gets the best next move\"\"\"\n",
    "        self.last_evaluation: int | None  # type annotation for mypy\n",
    "        self.stats[-1][\"cache_hits\"] = 0\n",
    "        self.stats[-1][\"cache_tries\"] = 0\n",
    "        self.stats[-1][\"leaf_ctr\"] = 0\n",
    "        self.stats[-1][\"max_depth\"] = 0\n",
    "        self.stats[-1][\"total_depth\"] = 0\n",
    "\n",
    "        if self.is_king_endgame != self.check_king_endgame(board):\n",
    "            self.last_evaluation += self.get_endgame_evaluation_change(board)\n",
    "        # Calculate current evaluation\n",
    "        if self.last_evaluation is None:  # type: ignore\n",
    "            current_evaluation = self.full_evaluate(board)\n",
    "        else:\n",
    "            # Get current evaluation (after opponent move)\n",
    "            current_evaluation = self.evaluate(board, self.last_evaluation)\n",
    "\n",
    "        for depth in range(1, self.DEPTH + 1):\n",
    "            # Call minimax and get best move\n",
    "            future_evaluation, best_move, eval_depth = self.minimax(\n",
    "                board, depth, current_evaluation\n",
    "            )\n",
    "\n",
    "        # Debugging fail save\n",
    "        assert best_move, f\"\"\"\n",
    "        Best move is None with fen '{board.fen()}' at player {type(self).__name__}! \n",
    "        depth: {self.DEPTH}, last_eval: {self.last_evaluation}, current_evaluation: {current_evaluation},\n",
    "        is_king_engame: {getattr(self, 'is_king_endgame', \"N/A\")}, move_stack: {board.move_stack}\n",
    "        \"\"\"\n",
    "        # Update last evaluation (after player move)\n",
    "        self.last_evaluation = current_evaluation + self.incremental_evaluate(\n",
    "            board, best_move\n",
    "        )\n",
    "        # Update stats\n",
    "        self.stats[-1][\"minimax_eval\"] = future_evaluation\n",
    "        self.stats[-1][\"board_eval_before_move\"] = current_evaluation\n",
    "        self.stats[-1][\"board_eval_after_move\"] = self.last_evaluation\n",
    "        self.stats[-1][\"eval_depth\"] = eval_depth\n",
    "        self.stats[-1][\"avg_depth\"] = (\n",
    "            self.stats[-1][\"total_depth\"] / (self.stats[-1][\"leaf_ctr\"] or 1)\n",
    "        )\n",
    "\n",
    "        if board.is_irreversible(best_move):\n",
    "            self.cache.clear()\n",
    "            gc.collect()\n",
    "            self.stats[-1][\"cache_cleared\"] = True\n",
    "        else:\n",
    "            self.stats[-1][\"cache_cleared\"] = False\n",
    "        self.stats[-1][\"cache_size_mb\"] = round(\n",
    "            sys.getsizeof(self.cache) / (1024 * 1024), 2\n",
    "        )\n",
    "        return best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Bereich\n",
    "\n",
    "Die folgenden Zellen enthalten Unit-Tests der oben implementierten Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Exercise03AI as Exercise03AITests\n",
    "import Exercise04AI as Exercise04AITests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create player and board\n",
    "unit_test_player = Exercise08AI(player_name=\"Ex08AI\", search_depth=2)\n",
    "board = chess.Board(\"5rk1/1b3p2/8/3p4/3p2P1/2Q4B/5P1K/R3R3 b - - 0 36\")\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test minimax\n",
    "def test_minimax(unit_test_player: ChessAI, board: chess.Board):\n",
    "    unit_test_player.cache = {}  # Clear cache\n",
    "    unit_test_player.stats[-1][\"cache_tries\"] = 0\n",
    "    unit_test_player.stats[-1][\"cache_hits\"] = 0\n",
    "    mm_evaluation, mm_move, mm_eval_depth = unit_test_player.minimax(\n",
    "        board, depth=2, current_evaluation=1240\n",
    "    )\n",
    "    print(f\"Minimax Evaluation: {mm_evaluation}\")\n",
    "    print(f\"Minimax Move: {mm_move}\")\n",
    "    print(f\"Minimax Evaluation Depth: {mm_eval_depth}\")\n",
    "    assert mm_evaluation == 325, \"Minimax evaluation does not match expected value!\"\n",
    "    assert mm_move.uci() == \"d4c3\", \"Minimax move does not match expected value!\"\n",
    "    assert unit_test_player.cache != {}, \"Cache is empty!\"\n",
    "    print(f\"Elements in cache: {len(unit_test_player.cache)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_minimax(unit_test_player, board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test next move function (with memoized minimax result)\n",
    "Exercise03AITests.test_next_move(unit_test_player, board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporärer Bereich\n",
    "\n",
    "Der folgende Bereich dient zum temporären Debuggen und kann nicht-funktionierenden Code enthalten. Dieser Bereich wird vor der Abgabe entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
