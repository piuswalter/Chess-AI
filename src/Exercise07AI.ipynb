{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".container {\n",
    "  width: 100%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from Exercise06AI import Exercise06AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 07: Minimax mit Alpha-Beta-Pruning, Memoisierung und Progressive Deepening\n",
    "\n",
    "Dieses Notebook erweitert den Minimax-Algorithmus mit Alpha-Beta-Pruning und Memoisierung um das Progressive Deepening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess_custom as chess\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class Exercise07AI(Exercise06AI):\n",
    "    \"\"\"Chooses middle game moves using minimax algorithm, alpha-beta-pruning, memoization and progressive deepening\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.local_cache: dict[tuple, int] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Deepening\n",
    "\n",
    "Da das in `Exercise05AI` implementierte Alpha-Beta-Pruning am effizientesten ist, wenn zuerst die besten Züge untersucht werden, wird nun die Liste der mögliche Züge vorab nach der bisher bekannten Evaluierung jedes Zuges sortiert. Hierzu wurde die Klasse um eine Variable `local_cache` erweitert, welche bisherige Evaluierungen speichert. Initial ist `local_cache` ein leeres Dictionary, welches als Schlüssel ein Tupel annimmt und als Wert eine Ganzzahl zurückgibt. Das Tupel besteht dabei aus der Tiefe und einem Zug. Die zugehörige Evaluierung dieses Zuges mit der gegebenen Tiefe wird als Value in diesem Dictionary gespeichert.\n",
    "\n",
    "Beim Progressive Deepening werden die Züge iterativ bis zu einer gegebenen Tiefe `DEPTH` berechnet. Dabei wird mit der Tiefe `1` begonnen um eine grobe Wertung der bis dahin möglichen Züge zu erhalten. Beim nächsten Aufruf mit Tiefe `2` kann dann die vorherige Evaluierung genutzt werden, um die Züge anhand diesem Maß zu sortieren und somit das Pruning signifikant zu erhöhen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `minimax` wird um eine mit [heapq](https://docs.python.org/3/library/heapq.html) implementierte Prioritäts-Warteschlange `moves` erweitert, welche alle verfügbaren Züge speichert. Als Priorität wird hierbei die Evaluierung des Zuges mit geringerer Tiefe verwendet. Falls keine Evaluierung vorhanden ist, wird eine fortlaufende Zugnummer als Priorität verwendet. Bedingt durch die Funktionsweise der Sortierung von `heapq` wird nun ein Tripel der Form\n",
    "\n",
    "$$ key = (-evaluation, i, move) $$\n",
    "\n",
    "erstellt. Hierbei wird $evaluation$ invertiert, da bei `heapq` der niedrigste Wert die höchste Priorität hat. Zusätzlich wird an zweiter Position eine fortlaufende Zugnummer $i$ hinzufügt. Dies ist notwendig, da die Evaluierung für zwei Züge gleich sein kann und `heapq` in diesem Fall die nächsten Elemente des Tupels vergleicht. Für $move$ ist das nicht möglich, daher wird zuvor $i$ eingefügt um diesen Vergleich (und die ansonsten entstehende `Exception`) zu verhindern. Die Berechnung der neuen Evaluierung erfolgt nun in zwei Schritten:\n",
    "\n",
    "1. Die in `board.legal_moves` verfügbaren Züge werden iterativ gegen den `local_cache` geprüft. Wenn die Evaluierung des Zuges bereits mit einer Tiefe von `depth - 2` verfügbar ist, wird diese genutzt, ansonsten wird die Zugnummer $i$ verwendet. Anschließend wird der Zug mit dem erhaltenen Wert als Priorität in die Warteschlange einsortiert.\n",
    "2. An dieser Stelle sind alle möglichen Züge bereits in der Liste `moves` enthalten und der Zug mit der bisher besten Evaluierung kann mithilfe von `heapq.heappop(moves)[2]` extrahiert werden. Nach der Berechnung durch den Minimax-Alogrithmus wird das neue Ergebnis im `local_cache` gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import heapq\n",
    "\n",
    "\n",
    "class Exercise07AI(Exercise07AI):  # type: ignore\n",
    "    @Exercise06AI.memoize_minimax\n",
    "    def minimax(\n",
    "        self,\n",
    "        board: chess.Board,\n",
    "        depth: int,\n",
    "        current_evaluation: int,\n",
    "        alpha: int = -Exercise07AI.LIMIT,\n",
    "        beta: int = Exercise07AI.LIMIT,\n",
    "    ) -> tuple[int, chess.Move]:\n",
    "        \"\"\"Searches the best value with given depth using minimax algorithm\"\"\"\n",
    "        early_abort_evaluation = self.minimax_early_abort(\n",
    "            board, depth, current_evaluation\n",
    "        )\n",
    "        if early_abort_evaluation is not None:\n",
    "            return early_abort_evaluation, None\n",
    "\n",
    "        best_move = None\n",
    "\n",
    "        # White to play (positive numbers are good)\n",
    "        moves: list[tuple[int, int, chess.Move]] = []\n",
    "        if board.turn:\n",
    "            for i, move in enumerate(board.legal_moves):\n",
    "                old_eval = self.local_cache.get((depth - 2, move), i)\n",
    "                heapq.heappush(moves, (-old_eval, i, move))\n",
    "            maxEvaluation = alpha\n",
    "            while moves:\n",
    "                move = heapq.heappop(moves)[2]\n",
    "                board.push(move)\n",
    "                evaluation, _ = self.minimax(\n",
    "                    board,\n",
    "                    depth - 1,\n",
    "                    self.evaluate(board, current_evaluation),\n",
    "                    maxEvaluation,\n",
    "                    beta,\n",
    "                )\n",
    "                self.local_cache[(depth - 1, move)] = evaluation\n",
    "                board.pop()\n",
    "                if evaluation >= beta:\n",
    "                    return evaluation, move\n",
    "                if depth == self.DEPTH and evaluation > maxEvaluation:\n",
    "                    best_move = move\n",
    "                maxEvaluation = max(maxEvaluation, evaluation)\n",
    "            return maxEvaluation, best_move\n",
    "\n",
    "        # Black to play (negative numbers are good)\n",
    "        else:\n",
    "            for i, move in enumerate(board.legal_moves):\n",
    "                old_eval = self.local_cache.get((depth - 2, move), i)\n",
    "                heapq.heappush(moves, (old_eval, i, move))\n",
    "            minEvaluation = beta\n",
    "            while moves:\n",
    "                move = heapq.heappop(moves)[2]\n",
    "                board.push(move)\n",
    "                evaluation, _ = self.minimax(\n",
    "                    board,\n",
    "                    depth - 1,\n",
    "                    self.evaluate(board, current_evaluation),\n",
    "                    alpha,\n",
    "                    minEvaluation,\n",
    "                )\n",
    "                self.local_cache[(depth - 1, move)] = evaluation\n",
    "                board.pop()\n",
    "                if evaluation <= alpha:\n",
    "                    return evaluation, move\n",
    "                if depth == self.DEPTH and evaluation < minEvaluation:\n",
    "                    best_move = move\n",
    "                minEvaluation = min(minEvaluation, evaluation)\n",
    "            return minEvaluation, best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `get_next_middle_game_move` wurde um eine `for`-Schleife zur iterativen Tiefensuche erweitert.\n",
    "Mithilfe dieser Schleife werden die Züge beginnend bei der Tiefe `1` bis zum konfigurierten Limit `DEPTH` berechnet. Beim jeweils nächsten Aufruf kann dann die vorherige Evaluierung aus dem Cache genutzt werden, um die Züge anhand diesem Maß zu sortieren und somit das Pruning signifikant zu erhöhen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise07AI(Exercise07AI):  # type: ignore\n",
    "    def get_next_middle_game_move(self, board: chess.Board) -> chess.Move:\n",
    "        \"\"\"Gets the best next move\"\"\"\n",
    "        self.last_evaluation: int | None  # type annotation for mypy\n",
    "        self.stats[-1][\"cache_tries\"] = 0\n",
    "        self.stats[-1][\"cache_hits\"] = 0\n",
    "\n",
    "        if self.is_king_endgame != self.check_king_endgame(board):\n",
    "            self.last_evaluation += self.get_endgame_evaluation_change(board)\n",
    "        # Calculate current evaluation\n",
    "        if self.last_evaluation is None:  # type: ignore\n",
    "            current_evaluation = self.full_evaluate(board)\n",
    "        else:\n",
    "            # Get current evaluation (after opponent move)\n",
    "            current_evaluation = self.evaluate(board, self.last_evaluation)\n",
    "\n",
    "        for depth in range(1, self.DEPTH + 1):\n",
    "            # Call minimax and get best move\n",
    "            future_evaluation, best_move = self.minimax(board, depth, current_evaluation)\n",
    "\n",
    "        # Reset local cache\n",
    "        self.local_cache: dict[tuple, int] = {}\n",
    "\n",
    "        # Debugging fail save\n",
    "        assert best_move, f\"\"\"\n",
    "        Best move is None with fen '{board.fen()}' at player {type(self).__name__}! \n",
    "        depth: {self.DEPTH}, last_eval: {self.last_evaluation}, current_evaluation: {current_evaluation},\n",
    "        is_king_engame: {getattr(self, 'is_king_endgame', \"N/A\")}, move_stack: {board.move_stack}\n",
    "        \"\"\"\n",
    "        # Update last evaluation (after player move)\n",
    "        self.last_evaluation = current_evaluation + self.incremental_evaluate(\n",
    "            board, best_move\n",
    "        )\n",
    "        # Update stats\n",
    "        self.stats[-1][\"minimax_eval\"] = future_evaluation\n",
    "        self.stats[-1][\"board_eval_before_move\"] = current_evaluation\n",
    "        self.stats[-1][\"board_eval_after_move\"] = self.last_evaluation\n",
    "        return best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Bereich\n",
    "\n",
    "Die folgenden Zellen enthalten Unit-Tests der oben implementierten Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Exercise03AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create player and board\n",
    "unit_test_player = Exercise07AI(player_name=\"Ex07AI\", search_depth=2)\n",
    "board = chess.Board(\"5rk1/1b3p2/8/3p4/3p2P1/2Q4B/5P1K/R3R3 b - - 0 36\")\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test minimax\n",
    "Exercise03AI.test_minimax(unit_test_player, board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test next move function\n",
    "Exercise03AI.test_next_move(unit_test_player, board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporärer Bereich\n",
    "\n",
    "Der folgende Bereich dient zum temporären Debuggen und kann nicht-funktionierenden Code enthalten. Dieser Bereich wird vor der Abgabe entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
