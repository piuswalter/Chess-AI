{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".container {\n",
    "  width: 100%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from Exercise04AI import Exercise04AI\n",
    "from Exercise06AI import Exercise06AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 07: Minimax mit Alpha-Beta-Pruning, Memoisierung und Progressive Deepening\n",
    "\n",
    "Dieses Notebook erweitert den Minimax-Algorithmus mit Alpha-Beta-Pruning und Memoisierung um das Progressive Deepening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class Exercise07AI(Exercise06AI):\n",
    "    \"\"\"Chooses middle game moves using minimax algorithm, alpha-beta-pruning, memoization and progressive deepening\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Deepening\n",
    "\n",
    "Das in `Exercise05AI` implementierte Alpha-Beta-Pruning ist deutlich effizienter (aber nicht zwingend optimal), wenn zuerst die besten Züge untersucht werden [9]. Aus diesem Grund wird nun die Liste der möglichen Züge vorab nach der bisher bekannten Evaluierung jedes Zuges sortiert. Die bisherigen Evaluierungen werden dabei aus dem, in `Exercise06AI` implementierten, Memoisierungs-Cache abgerufen. Wurde ein Zug bereits evaluiert, so wird das Tripel $(\\textrm{flag}, v, m)$ aus dem Cache zurückgegeben, wobei $v$ eine Annäherung an die Evaluierung des Zuges und $m$ der Zug ist.\n",
    "\n",
    "Die iterative Tiefensuche wurde ursprünglich verwendet, um die grundlegende zeitliche Kontrolle über Programm zu erhalten, da nach jeder Iteration die Suche abgebrochen werden kann, sofern ein zeitliches Limit überschritten wurde. In Kombination mit Alpha-Beta-Pruning ist die iterative Tiefensuche jedoch schneller als wenn direkt nach dem Ergebnis gesucht wird, da durch die Sortierung der Züge das Alpha-Beta-Fenster schneller verkleinert und somit mehr Knoten im Suchbaum übersprungen werden können[9].\n",
    "\n",
    "Wie in [10] beschrieben, werden beim Progressive Deepening die Züge iterativ bis zu einer gegebenen Tiefe `DEPTH` berechnet. Dabei wird mit der Tiefe `1` begonnen um eine grobe Wertung der bis dahin möglichen Züge zu erhalten. Beim nächsten Aufruf mit Tiefe `2` kann dann die vorherige Evaluierung genutzt werden, um die Züge anhand diesem Maß zu sortieren und somit das Pruning signifikant zu erhöhen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `minimax` wird um eine mit [heapq](https://docs.python.org/3/library/heapq.html) implementierte Prioritäts-Warteschlange `moves` erweitert, welche alle verfügbaren Züge speichert. Als Priorität wird hierbei die Evaluierung des Zuges mit geringerer Tiefe verwendet. Falls keine Evaluierung vorhanden ist, wird eine fortlaufende Zugnummer als Priorität verwendet. Bedingt durch die Funktionsweise der Sortierung von `heapq` wird nun ein Tripel der Form\n",
    "\n",
    "$$ \\textrm{key} = (\\textrm{evaluation}, \\textrm{i}, \\textrm{move}) $$\n",
    "\n",
    "erstellt. Hierbei wird für den weißen Spieler $evaluation$ invertiert, da bei `heapq` der niedrigste Wert die höchste Priorität hat und bei der Maximierung eine umgekehrte Priorisierung erforderlich ist. Zusätzlich wird an zweiter Position eine fortlaufende Zugnummer $i$ hinzufügt. Dies ist notwendig, da die Evaluierung für zwei Züge gleich sein kann und `heapq` in diesem Fall die nächsten Elemente des Tupels vergleicht. Für $move$ ist das nicht möglich, daher wird zuvor $i$ eingefügt um diesen Vergleich (und die ansonsten entstehende `Exception`) zu verhindern. Die Berechnung der neuen Evaluierung erfolgt nun in zwei Schritten:\n",
    "\n",
    "1. Die in `board.legal_moves` verfügbaren Züge werden iterativ gegen den Cache geprüft. Wenn die Evaluierung des Zuges bereits mit einer Tiefe von `depth - 2` verfügbar ist, wird diese genutzt, ansonsten wird die Zugnummer $i$ verwendet. Anschließend wird der Zug mit dem erhaltenen Wert (evtl. invertiert) als Priorität in die Warteschlange einsortiert.\n",
    "2. An dieser Stelle sind alle möglichen Züge bereits in der Liste `moves` enthalten und der Zug mit der bisher besten Evaluierung kann mithilfe von `heapq.heappop(moves)[2]` extrahiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import heapq\n",
    "\n",
    "\n",
    "class Exercise07AI(Exercise07AI):  # type: ignore\n",
    "    @Exercise06AI.memoize_minimax\n",
    "    def minimax(\n",
    "        self,\n",
    "        board: chess.Board,\n",
    "        depth: int,\n",
    "        current_evaluation: int,\n",
    "        alpha: int = -Exercise07AI.LIMIT,\n",
    "        beta: int = Exercise07AI.LIMIT,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        \"\"\"Searches the best value with given depth using minimax algorithm\"\"\"\n",
    "        self.stats[-1][\"nodes\"] += 1\n",
    "        early_abort_evaluation = self.minimax_early_abort(\n",
    "            board, depth, current_evaluation\n",
    "        )\n",
    "        if early_abort_evaluation is not None:\n",
    "            return early_abort_evaluation, None\n",
    "\n",
    "        best_move = None\n",
    "\n",
    "        # White to play (positive numbers are good)\n",
    "        moves: list[tuple[int, int, chess.Move]] = []\n",
    "        if board.turn:\n",
    "            for i, move in enumerate(board.legal_moves):\n",
    "                board.push(move)\n",
    "                key = Exercise04AI.get_key(board, depth - 2)\n",
    "                board.pop()\n",
    "                old_eval = self.cache.get(key, (None, i, None))[1]\n",
    "                heapq.heappush(moves, (-old_eval, i, move))\n",
    "            maxEvaluation = alpha\n",
    "            while moves:\n",
    "                move = heapq.heappop(moves)[2]\n",
    "                new_evaluation = self.incremental_evaluate(\n",
    "                    board, current_evaluation, move\n",
    "                )\n",
    "                board.push(move)\n",
    "                evaluation, _ = self.minimax(\n",
    "                    board,\n",
    "                    depth - 1,\n",
    "                    new_evaluation,\n",
    "                    maxEvaluation,\n",
    "                    beta,\n",
    "                )\n",
    "                board.pop()\n",
    "                if evaluation >= beta:\n",
    "                    return evaluation, move\n",
    "                if depth == self.DEPTH and evaluation > maxEvaluation:\n",
    "                    best_move = move\n",
    "                maxEvaluation = max(maxEvaluation, evaluation)\n",
    "            return maxEvaluation, best_move\n",
    "\n",
    "        # Black to play (negative numbers are good)\n",
    "        else:\n",
    "            for i, move in enumerate(board.legal_moves):\n",
    "                board.push(move)\n",
    "                key = Exercise04AI.get_key(board, depth - 2)\n",
    "                board.pop()\n",
    "                old_eval = self.cache.get(key, (None, i, None))[1]\n",
    "                heapq.heappush(moves, (old_eval, i, move))\n",
    "            minEvaluation = beta\n",
    "            while moves:\n",
    "                move = heapq.heappop(moves)[2]\n",
    "                new_evaluation = self.incremental_evaluate(\n",
    "                    board, current_evaluation, move\n",
    "                )\n",
    "                board.push(move)\n",
    "                evaluation, _ = self.minimax(\n",
    "                    board,\n",
    "                    depth - 1,\n",
    "                    new_evaluation,\n",
    "                    alpha,\n",
    "                    minEvaluation,\n",
    "                )\n",
    "                board.pop()\n",
    "                if evaluation <= alpha:\n",
    "                    return evaluation, move\n",
    "                if depth == self.DEPTH and evaluation < minEvaluation:\n",
    "                    best_move = move\n",
    "                minEvaluation = min(minEvaluation, evaluation)\n",
    "            return minEvaluation, best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `get_next_middle_game_move` wurde um eine `for`-Schleife zur iterativen Tiefensuche erweitert.\n",
    "Mithilfe dieser Schleife werden die Züge beginnend bei der Tiefe `1` bis zum konfigurierten Limit `DEPTH` berechnet. Beim jeweils nächsten Aufruf kann dann die vorherige Evaluierung aus dem Cache genutzt werden, um die Züge anhand ihrer Evaluierung zu sortieren und somit das Pruning signifikant zu erhöhen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "class Exercise07AI(Exercise07AI):  # type: ignore\n",
    "    def get_next_middle_game_move(self, board: chess.Board) -> chess.Move:\n",
    "        \"\"\"Gets the best next move\"\"\"\n",
    "        self.last_evaluation: int | None  # type annotation for mypy\n",
    "        self.stats[-1][\"cache_tries\"] = 0\n",
    "        self.stats[-1][\"cache_hits\"] = 0\n",
    "\n",
    "        if self.is_king_endgame != self.check_king_endgame(board):\n",
    "            self.last_evaluation += self.get_endgame_evaluation_change(board)\n",
    "        # Calculate current evaluation\n",
    "        if self.last_evaluation is None:  # type: ignore\n",
    "            current_evaluation = self.full_evaluate(board)\n",
    "        else:\n",
    "            # Get current evaluation (after opponent move)\n",
    "            last_move = board.pop()\n",
    "            current_evaluation = self.incremental_evaluate(board, self.last_evaluation, last_move)  # type: ignore\n",
    "            board.push(last_move)\n",
    "\n",
    "        self.stats[-1][\"nodes\"] = 0\n",
    "        for depth in range(1, self.DEPTH + 1):\n",
    "            # Call minimax and get best move\n",
    "            future_evaluation, best_move = self.minimax(\n",
    "                board, depth, current_evaluation\n",
    "            )\n",
    "\n",
    "        # Debugging fail save\n",
    "        assert best_move, f\"\"\"\n",
    "        Best move is None with fen '{board.fen()}' at player {type(self).__name__}! \n",
    "        depth: {self.DEPTH}, last_eval: {self.last_evaluation}, current_evaluation: {current_evaluation},\n",
    "        is_king_engame: {getattr(self, 'is_king_endgame', \"N/A\")}, move_stack: {board.move_stack}\n",
    "        \"\"\"\n",
    "        # Update last evaluation (after player move)\n",
    "        self.last_evaluation = self.incremental_evaluate(\n",
    "            board, current_evaluation, best_move\n",
    "        )\n",
    "        # Update stats\n",
    "        self.stats[-1][\"minimax_eval\"] = future_evaluation\n",
    "        self.stats[-1][\"board_eval_before_move\"] = current_evaluation\n",
    "        self.stats[-1][\"board_eval_after_move\"] = self.last_evaluation\n",
    "\n",
    "        if board.is_irreversible(best_move):\n",
    "            self.cache.clear()\n",
    "            self.stats[-1][\"cache_cleared\"] = True\n",
    "        else:\n",
    "            self.stats[-1][\"cache_cleared\"] = False\n",
    "        self.stats[-1][\"cache_size_mb\"] = round(\n",
    "            sys.getsizeof(self.cache) / (1024 * 1024), 2\n",
    "        )\n",
    "        return best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Bereich\n",
    "\n",
    "Die folgenden Zellen enthalten Unit-Tests der oben implementierten Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Exercise04AI as Exercise04AI_\n",
    "import Exercise02AI as Exercise02AI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create player and board\n",
    "unit_test_player = Exercise07AI(player_name=\"Ex07AI\", search_depth=3)\n",
    "board = chess.Board(\"5rk1/1b3p2/8/3p4/3p2P1/2Q4B/5P1K/R3R3 b - - 0 36\")\n",
    "board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu validieren, dass das implementierte Progressive Deepening die Evaluierung des berechneten Ergebnis nicht verändert hat, wird die Minimax-Testfunktion der `Exercise04AI` aufgerufen, wobei dieselben Evaluierungsparameter wie in den vorhergehenden KI-Versionen verwendet werden. Die Anzahl der erwarteten Cache-Anfragen und berechneten Knotenpunkten ist durch das Progressive Deepening höher als in der `Exercise06AI`, durch das verwendete Alpha-Beta-Pruning allerdings niedriger als in der `Exercise04AI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test minimax\n",
    "Exercise04AI_.test_minimax(\n",
    "    unit_test_player,\n",
    "    board,\n",
    "    current_evaluation=1240,\n",
    "    expected_evaluation=325,\n",
    "    expected_move=\"d4c3\",\n",
    "    expected_nodes=5553,\n",
    "    expected_cache_tries=7487,\n",
    "    expected_cache_hits=2560,\n",
    "    expected_cache_elements=4927,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Funktion `get_next_middle_game_move` zu testen, wird überprüft, ob diese korrekterweise den Zug zurückgibt, in welchem die Dame durch den schwarzen Bauern geschlagen wird. Durch das Progressive Deepening steigt die Zahl der berechneten Knoten auf $81$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test next move function (with memoized minimax result)\n",
    "Exercise02AI_.test_next_move(\n",
    "    unit_test_player,\n",
    "    board,\n",
    "    expected_move=\"d4c3\",\n",
    "    expected_nodes=81,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporärer Bereich\n",
    "\n",
    "Der folgende Bereich dient zum temporären Debuggen und kann nicht-funktionierenden Code enthalten. Dieser Bereich wird vor der Abgabe entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
