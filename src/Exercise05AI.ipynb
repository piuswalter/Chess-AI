{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".container {\n",
    "  width: 100%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import nbimporter\n",
    "from typing import Any\n",
    "from Exercise03AI import Exercise03AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 05: Minimax mit Alpha-Beta-Pruning\n",
    "\n",
    "Dieses Notebook implementiert den Minimax-Algorithmus mit Alpha-Beta-Pruning (ohne Memoisierung). Hierzu wird die `minimax`-Funktion adaptiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise05AI(Exercise03AI):\n",
    "    \"\"\"Chooses middle game moves using minimax algorithm and alpha-beta-pruning.\"\"\"\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha-Beta Pruning\n",
    "\n",
    "Alpha-Beta Pruning ist eine Suchtechnik aus den späteren 1950er Jahren, bei der große Teile des Suchbaumes übersprungen werden können[9]. Gegenüber dem bisherigen Minimax werden hierfür die Parameter $\\alpha$ und $\\beta$ hinzugefügt, die angeben, welches Ergebnis die Spieler bei optimaler Spielweise erreichen können. Mithilfe dieser Parameter kann entschieden werden, welche Teile des Suchbaumes nicht untersucht werden müssen, da sie nicht zum Ergebnis beitragen[9]. Die folgenden Gleichungen beschreiben das Verhalten der neuen Funktion $\\textrm{alphaBetaMinimax}$ gegenüber der bisherigen $\\textrm{minimax}$-Funktion[2]. Hierbei ist $s$ der aktuelle Spielzustand (Board) und $d \\in \\mathbb{N}$ die gewählte Suchtiefe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "&\\alpha \\le \\textrm{minimax}(s,d) \\le \\beta &&\\implies \\textrm{alphaBetaMinimax}(s,d,\\alpha,\\beta) = \\textrm{minimax}(s,d)\\\\\n",
    "&\\textrm{minimax}(s,d) < \\alpha &&\\implies \\textrm{alphaBetaMinimax}(s,d,\\alpha,\\beta) \\le \\alpha\\\\\n",
    "&\\beta < \\textrm{minimax}(s,d) &&\\implies \\beta \\le \\textrm{alphaBetaMinimax}(s,d,\\alpha,\\beta)\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dieser Spezifikation kann der Aufruf $\\textrm{minimax}(s,d)$ durch den Aufruf $\\textrm{alphaBetaMinimax}(s,d,-\\textrm{LIMIT},\\textrm{LIMIT})$ ersetzt werden. Hierbei ist $\\textrm{LIMIT}$ die obere bzw. untere Grenze für alle möglichen Evaluierungen. In dieser Implementierung wurde das Limit als $99999$ bzw. $-99999$ gewählt. Für die Rückwärtskompatibilität wurde der Funktionsname $\\textrm{minimax}$ in der folgenden Implementierung beibehalten und überschreibt die Funktion der Oberklasse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/AlphaBetaPruning.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das o.g. Bild aus [A2] zeigt das Überspringen oder Abschneiden von Teilbäumen, wenn dort keine bessere Beurteilung zu erwarten ist. In diesem Bild wird von links nach rechts und von unten nach oben ausgewertet. Die grau dargestellten Beurteilungen werden in der Praxis nicht berechnet und sind nur für ein besseres Verständnis enthalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax (aktualisiert)\n",
    "\n",
    "Die rekursive `minimax`-Funktion betrachtet bei einem gegebenen Board alle möglichen Stellungen (Boards) nach einer gegebenen Anzahl von Halbzügen (Tiefe) und gibt die beste Evaluierung (`evaluate`-Funktion) für den aktuellen Spieler zurück. Hierfür bekommt die Funktion als Argumente das aktuelle Board (`board`), die Evaluierungstiefe in Halbzügen (`depth`), die Evaluierung des übergebenen Boards (`current_evaluation`), eine untere Grenze für die Evaluierung (`alpha`) und eine obere Grenze für die Evaluierung (`beta`). Zurückgegeben wird ein Tupel bestehend aus der besten Evaluierung und des Halbzuges, der auf den Pfad zu dieser Evaluierung führt.\n",
    "\n",
    "Die folgende Implementierung des Minimax-Algorithmus ist zu großen Teilen mit der von `Exercise02AI` identisch. Als Neuerung wurde Alpha-Beta-Pruning hinzugefügt. Im Folgenden werden nur die Änderungen betrachtet:\n",
    "\n",
    "1. Die `minimax`-Funktion bekommt zwei zusätzliche Parameter $\\textrm{alpha}$ und $\\textrm{beta}$ welche mithilfe von Standardwerten beim ersten Aufruf auf das positive bzw. negative Limit gesetzt werden.\n",
    "2. Wenn der weiße Spieler am Zug ist (Maximierung) wird `maxEvaluation` auf den Wert $\\textrm{alpha}$ gesetzt. Wie bisher wird nun jeder Zug evaluiert. Hierbei wird das Maximum der bisherigen Evaluationen jeweils als neue untere Grenze $\\textrm{alpha}$ eingesetzt. Falls eine Evaluierung größer oder gleich $\\textrm{beta}$ ist, wird die Suche abgebrochen (pruning) und diese Evaluierung zurückgegeben.\n",
    "2. Falls der schwarze Spieler am Zug ist (Minimierung) wird `minEvaluation` auf den Wert $\\textrm{beta}$ gesetzt. Wie bisher wird nun jeder Zug evaluiert. Hierbei wird das Minimum der bisherigen Evaluationen jeweils als neue obere Grenze $\\textrm{beta}$ eingesetzt. Falls eine Evaluierung kleiner oder gleich $\\textrm{alpha}$ ist, wird die Suche abgebrochen (pruning) und diese Evaluierung zurückgegeben.\n",
    "\n",
    "Für einen Aufruf mit Weiß am Zug gilt also: \n",
    "$$\\textrm{return_value} = \\begin{cases}\n",
    "\\textrm{alpha} \\quad \\texttt{falls} \\quad \\textrm{evaluation} < \\textrm{alpha} \\quad \\texttt{für alle} \\quad \\textrm{move} \\in \\textrm{board.legal_moves},\\\\\n",
    "\\ge \\textrm{beta} \\quad \\texttt{falls} \\quad \\textrm{move} \\in \\textrm{board.legal_moves} \\quad \\texttt{mit} \\quad \\textrm{evaluation} \\ge \\textrm{beta} \\quad \\texttt{existiert,}\\\\\n",
    "\\textrm{evaluation} \\quad \\texttt{sonst}.\n",
    "\\end{cases}$$\n",
    "Für einen Aufruf mit Schwarz am Zug gilt: \n",
    "$$\\textrm{return_value} = \\begin{cases}\n",
    "\\textrm{beta} \\quad \\texttt{falls} \\quad \\textrm{evaluation} > \\textrm{beta} \\quad \\texttt{für alle} \\quad \\textrm{move} \\in \\textrm{board.legal_moves},\\\\\n",
    "\\le \\textrm{alpha} \\quad \\texttt{falls} \\quad move \\in \\textrm{board.legal_moves} \\quad \\texttt{mit} \\quad \\textrm{evaluation} \\le \\textrm{alpha} \\quad \\texttt{existiert,}\\\\\n",
    "\\textrm{evaluation} \\quad \\texttt{sonst}.\n",
    "\\end{cases}$$\n",
    "Somit ist in beiden Fällen die oben gegebene formale Spezifikation für das Alpha-Beta-Pruning erfüllt und das Programm liefert dieselbe Auswertung wie die Implementierung in `Exercise02AI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise05AI(Exercise05AI):  # type: ignore\n",
    "    def minimax(\n",
    "        self,\n",
    "        board: chess.Board,\n",
    "        depth: int,\n",
    "        current_evaluation: int,\n",
    "        alpha: int = -Exercise05AI.LIMIT,\n",
    "        beta: int = Exercise05AI.LIMIT,\n",
    "    ) -> tuple[int, chess.Move | None]:\n",
    "        \"\"\"Searches the best value with a given depth using the minimax algorithm.\"\"\"\n",
    "        self.stats[-1][\"nodes\"] += 1\n",
    "        early_abort_evaluation = self.minimax_early_abort(board, depth, current_evaluation)\n",
    "        if early_abort_evaluation is not None:\n",
    "            return early_abort_evaluation, None\n",
    "\n",
    "        best_move = None\n",
    "\n",
    "        # White to play (positive numbers are good)\n",
    "        if board.turn:\n",
    "            maxEvaluation = alpha\n",
    "            for move in board.legal_moves:\n",
    "                new_evaluation = self.incremental_evaluate(board, current_evaluation, move)\n",
    "                board.push(move)\n",
    "                evaluation, _ = self.minimax(\n",
    "                    board,\n",
    "                    depth - 1,\n",
    "                    new_evaluation,\n",
    "                    maxEvaluation,\n",
    "                    beta,\n",
    "                )\n",
    "                board.pop()\n",
    "                if evaluation >= beta:\n",
    "                    return evaluation, move\n",
    "                if depth == self.DEPTH and evaluation > maxEvaluation:\n",
    "                    best_move = move\n",
    "                maxEvaluation = max(maxEvaluation, evaluation)\n",
    "            return maxEvaluation, best_move\n",
    "\n",
    "        # Black to play (negative numbers are good)\n",
    "        else:\n",
    "            minEvaluation = beta\n",
    "            for move in board.legal_moves:\n",
    "                new_evaluation = self.incremental_evaluate(board, current_evaluation, move)\n",
    "                board.push(move)\n",
    "                evaluation, _ = self.minimax(\n",
    "                    board,\n",
    "                    depth - 1,\n",
    "                    new_evaluation,\n",
    "                    alpha,\n",
    "                    minEvaluation,\n",
    "                )\n",
    "                board.pop()\n",
    "                if evaluation <= alpha:\n",
    "                    return evaluation, move\n",
    "                if depth == self.DEPTH and evaluation < minEvaluation:\n",
    "                    best_move = move\n",
    "                minEvaluation = min(minEvaluation, evaluation)\n",
    "            return minEvaluation, best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Bereich\n",
    "\n",
    "Die folgenden Zellen enthalten Unit-Tests der oben implementierten Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AIBaseClass import ChessAI\n",
    "import Exercise02AI as Exercise02AI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create player and board\n",
    "unit_test_player = Exercise05AI(player_name=\"Ex05AI\", search_depth=3)\n",
    "board = chess.Board(\"5rk1/1b3p2/8/3p4/3p2P1/2Q4B/5P1K/R3R3 b - - 0 36\")\n",
    "board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um das Alpha-Beta-Pruning zu testen, wird der Minimax-Unittest analog zur `Exercise03AI` aufgerufen. Während für den verwendeten Zug und die erwartete Evaluierung dieselben Werte verwendet werden, ist durch die `expected_nodes`-Metrik zu sehen, dass der Berechnungsaufwand sich von ursprünglich $14377$ auf $2788$ Knoten reduziert hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use minimax test without memoization\n",
    "Exercise02AI_.test_minimax(\n",
    "    unit_test_player,\n",
    "    board,\n",
    "    current_evaluation=1240,\n",
    "    expected_evaluation=325,\n",
    "    expected_move=\"d4c3\",\n",
    "    expected_nodes=2788,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
